{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_12_TXL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPixOPRoWc3mNxJMpjnfTYr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoanglam372000/Applied-Data-Science-Group-7/blob/nguyenthiman/8_12_TXL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Weea1SUCBmz"
      },
      "source": [
        "#### 1.1.1 Th∆∞ vi·ªán:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-20T19:01:05.711080Z",
          "iopub.status.busy": "2021-10-20T19:01:05.709811Z",
          "iopub.status.idle": "2021-10-20T19:01:05.712966Z",
          "shell.execute_reply": "2021-10-20T19:01:05.713505Z",
          "shell.execute_reply.started": "2021-10-20T18:55:02.236174Z"
        },
        "id": "b0c2dbd1",
        "papermill": {
          "duration": 0.05292,
          "end_time": "2021-10-20T19:01:05.713836",
          "exception": false,
          "start_time": "2021-10-20T19:01:05.660916",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc60b66-1910-4c37-d81d-f1b871d57561"
      },
      "source": [
        "!git clone https://github.com/hoanglam372000/Applied-Data-Science-Group-7.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Applied-Data-Science-Group-7' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-20T19:01:05.799619Z",
          "iopub.status.busy": "2021-10-20T19:01:05.798867Z",
          "iopub.status.idle": "2021-10-20T19:01:36.347459Z",
          "shell.execute_reply": "2021-10-20T19:01:36.346709Z",
          "shell.execute_reply.started": "2021-10-20T18:55:02.243151Z"
        },
        "id": "eff8fbcf",
        "papermill": {
          "duration": 30.592473,
          "end_time": "2021-10-20T19:01:36.347624",
          "exception": false,
          "start_time": "2021-10-20T19:01:05.755151",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8812021-1967-4488-aab4-4fa1687accac"
      },
      "source": [
        "!pip install emoji\n",
        "!pip install pyenchant\n",
        "#!pip install pycontractions"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.7/dist-packages (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-20T19:01:36.440628Z",
          "iopub.status.busy": "2021-10-20T19:01:36.439872Z",
          "iopub.status.idle": "2021-10-20T19:01:38.454775Z",
          "shell.execute_reply": "2021-10-20T19:01:38.454041Z",
          "shell.execute_reply.started": "2021-10-20T18:55:11.353568Z"
        },
        "id": "9b92498d",
        "papermill": {
          "duration": 2.064979,
          "end_time": "2021-10-20T19:01:38.454947",
          "exception": false,
          "start_time": "2021-10-20T19:01:36.389968",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d016d7-3898-4794-a011-b10b5d0f7d67"
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import emoji\n",
        "#import contractions\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#print(stopwords.words('english'))\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('names')\n",
        "names = nltk.corpus.names.words('male.txt')+nltk.corpus.names.words('female.txt')\n",
        "names_lower = [name.lower() for name in names]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK7e9Cf_q-AD",
        "outputId": "19f54892-adc5-4239-ceac-6d3af62c3c87"
      },
      "source": [
        "!pip install afinn"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: afinn in /usr/local/lib/python3.7/dist-packages (0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2inlf1hrDkj"
      },
      "source": [
        "from afinn import Afinn\n",
        "afinn = Afinn(language='en')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImKmbWuCU9Ve"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOZrwO3_VACX"
      },
      "source": [
        "# TXL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WG_adjiU9S2"
      },
      "source": [
        "# icon ph·ªï bi·∫øn\n",
        "icon_df=pd.read_csv('/content/Applied-Data-Science-Group-7/icon.csv')\n",
        "icon=icon_df['icon'].to_list()\n",
        "# icon"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MwGdq-AU0fZ"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "# class x·ª≠ l√Ω v·∫•n ƒë·ªÅ tr√πng k√Ω t·ª± trong t·ª´, v√≠ d·ª• yoooouuuu -> you, nooooo -> no.\n",
        "#s·ª≠ d·ª•ng pattern c·ªßa regular expression, tuy nhi√™n ph·∫£i qu√©t th√™m qua wordnet ƒë·ªÉ tr√°nh tr∆∞·ªùng h·ª£p nh∆∞ t·ª´ goose b·ªã chuy·ªÉn th√†nh gose (m·∫∑c d√π goose l√† t·ª´ ƒë√∫ng)\n",
        "class RepeatReplacer(object):\n",
        "    def __init__(self):\n",
        "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "        self.repl = r'\\1\\2\\3'\n",
        "    def replace(self, word):\n",
        "        if wordnet.synsets(word):\n",
        "            return word\n",
        "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
        "        if repl_word != word:\n",
        "            return self.replace(repl_word)\n",
        "        else:\n",
        "            return repl_word\n",
        "char_repeat_correct = RepeatReplacer()\n",
        "def preprocessing_word(word):\n",
        "  if word != emoji.demojize(word):\n",
        "    return emoji.demojize(word)\n",
        "  #word = contractions.fix(word)\n",
        "  #word = stemmer.stem(word)\n",
        "  temp=[ \"<html_tag>\",'<URL>','<HIDDEN>','<tag>',\"<hash_tag>\"]\n",
        "  if word in temp:\n",
        "    return word\n",
        "  #s·ªë theo t·ª´\n",
        "  if word.isdigit():  \n",
        "    return '<NUMBER>'\n",
        "  try: \n",
        "    float(word)\n",
        "    return '<NUMBER>'\n",
        "  except:\n",
        "    pass\n",
        "  #word=word.lower() # vi·∫øt th∆∞·ªùngth∆∞·ªùng\n",
        "  #spell_corrector = Speller(lang='en') #s·ª≠a ch√≠nh t·∫£ theo t·ª´ng t·ª´ \n",
        "  #word=spell_corrector(word)\n",
        "\n",
        "  # r√∫t g·ªçn theo t·ª´\n",
        "  #lemmatizer = WordNetLemmatizer()\n",
        "  #wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "  #pos_tagged = nltk.pos_tag(word.split())\n",
        "  \n",
        "  #word = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged]\n",
        "  #word = ' '.join(word)\n",
        "\n",
        "  #word=word.translate(str.maketrans('', '', string.punctuation)) # ch·∫•m c√¢u theo t·ª´ (v√¨ tr√°nh emoij)\n",
        "  if word in icon:\n",
        "    return word\n",
        "  else:\n",
        "    for icon_ in icon:\n",
        "      if icon_ in word:\n",
        "        return icon_\n",
        "  word = re.sub(pattern=r'[!\"#$%&\\'()*+,-\\./:;<=>?@\\[\\]^_`{|}~0-9√Ø¬ø¬Ω]+', repl='', string=word)\n",
        "  word = char_repeat_correct.replace(word)\n",
        "  word = stemmer.stem(word)\n",
        "  if (word in  names):\n",
        "    return '<Name>'\n",
        "  #word = stemmer.stem(word)\n",
        "  word = word.lower()\n",
        "  \n",
        "  if word==''or word ==' ':\n",
        "    return \"<Special_Character>\"\n",
        "  \n",
        "  if 'not' not in word and len(word.split())!=1:\n",
        "    return word.split()[0]+'_'\n",
        "  return word.replace(' ','_')\n",
        "\n",
        "def preprocessing_string(text):\n",
        "   # emoij\n",
        "   raw_text=emoji.demojize(text)\n",
        "   e_r=r':\\w+_\\w+:'\n",
        "   emoij_list=re.findall(e_r,raw_text)\n",
        "   for e in emoij_list:\n",
        "     raw_text=raw_text.replace(e,' ' + emoji.emojize(e) + ' ') # \"hi‚ù§Ô∏èüëç\"\n",
        "\n",
        "   #parser = BeautifulSoup(raw_text, \"html.parser\")\n",
        "   #raw_text = parser.get_text(separator = \"<html_tag>\")\n",
        "   raw_text = re.sub(pattern=r'https?://\\S+|www\\.\\S+', repl=' <URL> ', string=raw_text)\n",
        "   raw_text = re.sub(pattern='\\*{3,}?', repl=' <HIDDEN> ', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'@\\w+', repl=' <tag> ', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'#\\w+', repl=' <hash_tag> ', string=raw_text)\n",
        "  \n",
        "   s_=r\"[#!\\\"$%&\\'*+,-\\.?@\\[\\]():;/`{|}]+\"\n",
        "   for i in re.findall(s_,raw_text):\n",
        "     raw_text=raw_text.replace(i, ' '+i+' ')\n",
        "   new_tweet = raw_text.split()\n",
        "\n",
        "   word_list=raw_text.split()\n",
        "   word_list_=[]\n",
        "   for i in range(len(word_list)):\n",
        "     word_list[i]=preprocessing_word(word_list[i])\n",
        "     if word_list[i]!=\"<Special_Character>\":\n",
        "       word_list_.append(word_list[i])\n",
        "   if len(word_list)!=1:\n",
        "     word_list=word_list_\n",
        "   return word_list\n",
        "  \n",
        "def preprocessing_text(tweet):\n",
        "   raw_text = str(tweet).strip().replace('\\n', ' ')#.lower()\n",
        "   new_tweet = raw_text.split()\n",
        "   len_=len(new_tweet)\n",
        "   index2word = { w:new_tweet[w] for w in range(len_)}\n",
        "   word_list=raw_text.split()\n",
        "   return index2word,dict(zip([*range(len_)],map(preprocessing_string,word_list)))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ctrRct-VkeK",
        "outputId": "f0cfeb09-ed31-4dc2-fdb2-d286954e1850"
      },
      "source": [
        "# Test:\n",
        "tweet = ' My:)) mom just texted.......me and told *ship* James that Rodney3 charlie hi‚ù§Ô∏èüëç was√Ø¬ø¬Ω *** chasing?? and fireflies... in their :)) backyard. Awwwww I`m miss him!!! @hoanglam 123 ://    http://twitpic.com/66pn1'\n",
        "tweet2 = 'nooooooooo'\n",
        "index2word_temp,preprocessed_tweet_temp = preprocessing_text(tweet)\n",
        "print('index2word: ',index2word_temp)\n",
        "print('preprocessed_tweet: ',preprocessed_tweet_temp)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index2word:  {0: 'My:))', 1: 'mom', 2: 'just', 3: 'texted.......me', 4: 'and', 5: 'told', 6: '*ship*', 7: 'James', 8: 'that', 9: 'Rodney3', 10: 'charlie', 11: 'hi‚ù§Ô∏èüëç', 12: 'was√Ø¬ø¬Ω', 13: '***', 14: 'chasing??', 15: 'and', 16: 'fireflies...', 17: 'in', 18: 'their', 19: ':))', 20: 'backyard.', 21: 'Awwwww', 22: 'I`m', 23: 'miss', 24: 'him!!!', 25: '@hoanglam', 26: '123', 27: '://', 28: 'http://twitpic.com/66pn1'}\n",
            "preprocessed_tweet:  {0: ['my', ':))'], 1: ['mom'], 2: ['just'], 3: ['text', 'me'], 4: ['and'], 5: ['told'], 6: ['ship'], 7: ['jame'], 8: ['that'], 9: ['rodney'], 10: ['charli'], 11: ['hi', ':red_heart:', ':thumbs_up:'], 12: ['wa'], 13: ['<HIDDEN>'], 14: ['chase'], 15: ['and'], 16: ['firefli'], 17: ['in'], 18: ['their'], 19: [':))'], 20: ['backyard'], 21: ['aw'], 22: ['i', 'm'], 23: ['miss'], 24: ['him'], 25: ['<tag>'], 26: ['<NUMBER>'], 27: [':/'], 28: ['<URL>']}\n"
          ]
        }
      ]
    }
  ]
}
