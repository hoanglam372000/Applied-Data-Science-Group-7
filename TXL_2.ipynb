{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TXL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoanglam372000/Applied-Data-Science-Group-7/blob/nguyenthiman/TXL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0k7ve3K3Xq_"
      },
      "source": [
        "Ti·ªÅn x·ª≠ l√Ω bao g·ªìm:\n",
        "\n",
        "A. L√†m th√¥\n",
        "    X√≥a \\n \n",
        "\n",
        "    X√≥a nhi·ªÅu kho·∫£ng tr·∫Øng\n",
        "\n",
        "    x·ª≠ l√Ω html, url, tag, k√Ω t·ª± ·∫©n *** --> \n",
        "\n",
        "                 [ \"<html_tag>\",'<URL>','<HIDDEN>','<tag>',\"<hash_tag>\"]\n",
        "\n",
        "    s·ªë  --> <NUMBER>\n",
        "\n",
        "  M·ªü r·ªông don't --> do not\n",
        "\n",
        "  X·ª≠ l√Ω vi·∫øt t·∫Øt omg --> // ch∆∞a \n",
        "\n",
        "  emoij --> t·ª´ üëç --> :thumbs_up:\n",
        "\n",
        "  k√≠ t·ª± ƒë·∫∑c bi·ªát --> <Special_Character>\n",
        "\n",
        "  Vi·∫øt th∆∞·ªùng\n",
        "\n",
        "  s·ª≠a ch√≠nh t·∫£\n",
        "\n",
        "  lemma\n",
        "    \n",
        "        caring --> care\n",
        "\n",
        "  I'm not happy --> i am not not_happy (t√≠nh t·ª´, tr·∫°ng t·ª´) // ch∆∞a\n",
        "\n",
        "  t√™n --> <name>\n",
        "       \n",
        "\n",
        "G√°n nh√£n // ch∆∞a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVZJat2Kn2kq"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHdcb2ApRooW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7283e1-ca1b-40cd-d5d7-013956657784"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install emoji\n",
        "!pip install autocorrect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 284 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 321 kB 46.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85447 sha256=a81536db0d0fb8781f260e8c4184186e9dc1fc40dd9814d0e18c309a5cbc572a\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170 kB 7.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=318fe913a8fcc28e6d458e091b0088973e07c73aba314fa954e8eb456d571380\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n",
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.5.0.tar.gz (622 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 622 kB 7.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.5.0-py3-none-any.whl size=621851 sha256=37c503b7dc23a30432ea53eb37ea6f96722314b03253cde68ec999e59d73c305\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/8e/bd/f6fd900a056a031bf710a00bca338d86f43b83f0c25ab5242f\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q39imbskmtRX",
        "outputId": "e12bcdeb-df34-407a-f5b2-11965190055c"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import contractions\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('names')\n",
        "import emoji\n",
        "from bs4 import BeautifulSoup\n",
        "from autocorrect import Speller\n",
        "names = nltk.corpus.names.words('male.txt')+nltk.corpus.names.words('female.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alON4M2OmxSd"
      },
      "source": [
        "# TXTXL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOJSlBJ2m1vw"
      },
      "source": [
        "\n",
        "c√≤n t·ª´ vi·∫øt t·∫Øt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzX6QClT5lzj",
        "outputId": "d5ca9828-0bc0-4423-f54c-4260c6f1ef5e"
      },
      "source": [
        "tweet = \" #gamegame36 didn't hi‚ù§Ô∏èüëçMy mom just texted me and told me that Rodney was *** chasing?? and fireflies... in (their) :)) backyard. Awwwww I'm miss him!!! @hoanglam 123 ://    http://twitpic.com/66pn1\"\n",
        "tweet2 = 'theira :)) <3'\n",
        "def preprocessing_word(word):\n",
        "  if word != emoji.demojize(word):\n",
        "    return emoji.demojize(word)\n",
        "  word = contractions.fix(word)\n",
        "  temp=[ \"<html_tag>\",'<URL>','<HIDDEN>','<tag>',\"<hash_tag>\"]\n",
        "  if word in temp:\n",
        "    return word\n",
        "  #s·ªë theo t·ª´\n",
        "  if word.isdigit():  \n",
        "    return '<NUMBER>'\n",
        "  try: \n",
        "    float(word)\n",
        "    return '<NUMBER>'\n",
        "  except:\n",
        "    pass\n",
        "  if word in names:\n",
        "    return '<Name>'\n",
        "\n",
        "  word=word.lower() # vi·∫øt th∆∞·ªùngth∆∞·ªùng\n",
        "  spell_corrector = Speller(lang='en') #s·ª≠a ch√≠nh t·∫£ theo t·ª´ng t·ª´ \n",
        "  word=spell_corrector(word)\n",
        "\n",
        "  # r√∫t g·ªçn theo t·ª´\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "  pos_tagged = nltk.pos_tag(word.split())\n",
        "  \n",
        "  word = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged]\n",
        "  word = ' '.join(word)\n",
        "\n",
        "  word=word.translate(str.maketrans('', '', string.punctuation)) # ch·∫•m c√¢u theo t·ª´ (v√¨ tr√°nh emoij)\n",
        "  if word==''or word ==' ':\n",
        "    return \"<Special_Character>\"\n",
        "  if 'not' not in word and len(word.split())!=1:\n",
        "    return word.split()[0]+'_'\n",
        "  return word.replace(' ','_')\n",
        "\n",
        "def preprocessing_text(tweet):\n",
        "   raw_text = str(tweet).strip().replace('\\n', ' ')#.lower()\n",
        "   # emoij\n",
        "   raw_text=emoji.demojize(raw_text)\n",
        "   e_r=r':\\w+_\\w+:'\n",
        "   emoij_list=re.findall(e_r,raw_text)\n",
        "   for e in emoij_list:\n",
        "     raw_text=raw_text.replace(e,' ' + emoji.emojize(e) + ' ') # \"hi‚ù§Ô∏èüëç\"\n",
        "\n",
        "   new_tweet = raw_text.split()\n",
        "   len_=len(new_tweet)\n",
        "   index2word = { w:new_tweet[w] for w in range(len_)}\n",
        "   parser = BeautifulSoup(raw_text, \"html.parser\")\n",
        "   raw_text = parser.get_text(separator = \"<html_tag>\")\n",
        "   raw_text = re.sub(pattern=r'https?://\\S+|www\\.\\S+', repl='<URL>', string=raw_text)\n",
        "   raw_text = re.sub(pattern='\\*+', repl='<HIDDEN>', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'@\\w+', repl='<tag>', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'#\\w+', repl='<hash_tag>', string=raw_text)\n",
        "   word_list=raw_text.split()\n",
        "   \n",
        "   return index2word,dict(zip([*range(len_)],map(preprocessing_word,word_list)))\n",
        "  #  return list(map(preprocessing_word,word_list))\n",
        "   # (\" \".join([word_list]))\n",
        "\n",
        "index2word, p=preprocessing_text(tweet)\n",
        "print(index2word)\n",
        "print(p)\n",
        "# preprocessing_word(\"texted\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '#gamegame36', 1: \"didn't\", 2: 'hi', 3: '‚ù§Ô∏è', 4: 'üëç', 5: 'My', 6: 'mom', 7: 'just', 8: 'texted', 9: 'me', 10: 'and', 11: 'told', 12: 'me', 13: 'that', 14: 'Rodney', 15: 'was', 16: '***', 17: 'chasing??', 18: 'and', 19: 'fireflies...', 20: 'in', 21: '(their)', 22: ':))', 23: 'backyard.', 24: 'Awwwww', 25: \"I'm\", 26: 'miss', 27: 'him!!!', 28: '@hoanglam', 29: '123', 30: '://', 31: 'http://twitpic.com/66pn1'}\n",
            "{0: '<hash_tag>', 1: 'do_not', 2: 'hi', 3: ':red_heart:', 4: ':thumbs_up:', 5: 'my', 6: 'mom', 7: 'just', 8: 'test', 9: 'me', 10: 'and', 11: 'told', 12: 'me', 13: 'that', 14: '<Name>', 15: 'be', 16: '<HIDDEN>', 17: 'chasing', 18: 'and', 19: 'fireflies', 20: 'in', 21: 'their', 22: '<Special_Character>', 23: 'backyard', 24: 'awwwww', 25: 'i_', 26: 'miss', 27: 'him', 28: '<tag>', 29: '<NUMBER>', 30: '<Special_Character>', 31: '<URL>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buhyjSFGppng"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
