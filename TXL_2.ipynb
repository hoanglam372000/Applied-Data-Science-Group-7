{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TXL_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoanglam372000/Applied-Data-Science-Group-7/blob/nguyenthiman/TXL_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVZJat2Kn2kq"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHdcb2ApRooW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e75ddd-ae6d-4237-a873-944dfb28a5fc"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install emoji\n",
        "!pip install autocorrect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.52)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.7/dist-packages (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q39imbskmtRX",
        "outputId": "aa3ac256-e920-4dc1-f9d0-039863a05b52"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import contractions\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('names')\n",
        "import emoji\n",
        "from bs4 import BeautifulSoup\n",
        "from autocorrect import Speller\n",
        "names = nltk.corpus.names.words('male.txt')+nltk.corpus.names.words('female.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alON4M2OmxSd"
      },
      "source": [
        "# TXTXL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOJSlBJ2m1vw"
      },
      "source": [
        "còn nhiềnhiều ý:\n",
        "từ viết tắt\n",
        "\n",
        "emoij\n",
        "\n",
        "emoij bị dính  hi:))) :)<3\n",
        "\n",
        "kí tự đặc biệt (vì tránh emoij)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "kzX6QClT5lzj",
        "outputId": "59cb572d-9ed1-4dda-dd12-428ced83d248"
      },
      "source": [
        "tweet = \" #gamegame36 didn't My mom just texted me and told me that Rodney was *** chasing?? and fireflies... in (their) :)) backyard. Awwwww I'm miss him!!! @hoanglam 123 ://    http://twitpic.com/66pn1\"\n",
        "tweet2 = 'theira :)) <3'\n",
        "def preprocessing_word(word):\n",
        "  word = contractions.fix(word)\n",
        "  print(1,word)\n",
        "  temp=[ \"<html_tag>\",'<URL>','<HIDDEN>','<tag>',\"<hash_tag>\"]\n",
        "  if word in temp:\n",
        "    return word\n",
        "  #số theo từ\n",
        "  if word.isdigit():  \n",
        "    return '<NUMBER>'\n",
        "  try: \n",
        "    float(word)\n",
        "    return '<NUMBER>'\n",
        "  except:\n",
        "    pass\n",
        "  if word in names:\n",
        "    return '<Name>'\n",
        "  # emoij\n",
        "  # emoij dính  hi:))) :)<3\n",
        "\n",
        "  word=word.lower() # viết thườngthường\n",
        "  spell_corrector = Speller(lang='en') #sửa chính tả theo từng từ \n",
        "  word=spell_corrector(word)\n",
        "\n",
        "  # correct_words=[]\n",
        "  # spell_corrector = SpellChecker()\n",
        "  # misSpelled_words = spell_corrector.unknown(word.split())\n",
        "  # for each_word in word.split():\n",
        "  #   if each_word in misSpelled_words:\n",
        "  #     right_word = spell_corrector.correction(each_word)\n",
        "  #     correct_words.append(right_word)\n",
        "  #   else:\n",
        "  #     correct_words.append(each_word)\n",
        "  # word = ' '.join(correct_words)\n",
        "\n",
        "  print(2,word)\n",
        "  # print('correct',word)\n",
        "  # rút gọn theo từ\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "  pos_tagged = nltk.pos_tag(word.split())\n",
        "  \n",
        "  word = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged]\n",
        "  word = ' '.join(word)\n",
        "  print(3,word)\n",
        "  word=word.translate(str.maketrans('', '', string.punctuation)) # chấm câu theo từ (vì tránh emoij)\n",
        "  if word==''or word ==' ':\n",
        "    return \"<Special_Character>\"\n",
        "  if 'not' not in word and len(word.split())!=1:\n",
        "    return word.split()[0]+'_'\n",
        "  return word.replace(' ','_')\n",
        "\n",
        "def preprocessing_text(tweet):\n",
        "   raw_text = str(tweet).strip().replace('\\n', ' ')#.lower()\n",
        "   new_tweet = raw_text.split()\n",
        "   len_=len(new_tweet)\n",
        "   index2word = { w:new_tweet[w] for w in range(len_)}\n",
        "   print(index2word)\n",
        "   parser = BeautifulSoup(raw_text, \"html.parser\")\n",
        "   raw_text = parser.get_text(separator = \"<html_tag>\")\n",
        "   raw_text = re.sub(pattern=r'https?://\\S+|www\\.\\S+', repl='<URL>', string=raw_text)\n",
        "   raw_text = re.sub(pattern='\\*+', repl='<HIDDEN>', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'@\\w+', repl='<tag>', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'#\\w+', repl='<hash_tag>', string=raw_text)\n",
        "   word_list=raw_text.split()\n",
        "   \n",
        "   return dict(zip([*range(len_)],map(preprocessing_word,word_list)))\n",
        "  #  return list(map(preprocessing_word,word_list))\n",
        "   # (\" \".join([word_list]))\n",
        "\n",
        "# preprocessing_text(tweet)\n",
        "preprocessing_word(\"texted\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 texted\n",
            "2 tested\n",
            "3 test\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'test'"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ]
}