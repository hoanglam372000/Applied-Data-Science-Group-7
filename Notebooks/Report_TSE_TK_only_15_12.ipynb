{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoanglam372000/Applied-Data-Science-Group-7/blob/main/Notebooks/Report_TSE_TK_only_15_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4PnOCchUmrY"
      },
      "source": [
        "# Báo cáo đồ án môn \"Khoa học dữ liệu ứng dụng\"\n",
        "\n",
        "Nhóm 7:\n",
        "1. 18120429 – Phạm Trung Kiên - trungkien2000\n",
        "2. 18120431 – A Ly Ha Kim - kim-ali\n",
        "3. 18120434 – Thái Hoàng Lâm - hoanglam372000\n",
        "4. 18120462 – Nguyễn Thị Mận - ManCB26\n",
        "Link thùng chứa Github của nhóm:\n",
        "https://colab.research.google.com/github/hoanglam372000/Applied-Data-Science-Group-7"
      ],
      "id": "F4PnOCchUmrY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buOs0j5cul30"
      },
      "source": [
        "# I. Mô tả bài toán: \n",
        "## 1. Mô tả:\n",
        "Link: https://www.kaggle.com/c/tweet-sentiment-extraction \\\n",
        "\n",
        "- Bài toán thuộc lĩnh vực NLP, cụ thể là bài toán phân tích cảm xúc (Sentiment Analysis)\n",
        "- Không giống như những bài toán phân tích cảm xúc khác chỉ dừng ở việc phân lớp, bài toán yêu cầu phải rút trích những từ ngữ trong câu đóng góp cho cảm xúc tương ứng.\n",
        "- Ví dụ: \"I am happy\": positive -> \"happy\" \\\n",
        "\n",
        "## 2. Hướng giải quyết của nhóm \n",
        "Ban đầu nhóm định theo hướng giải quyết của người đứng hạng 3 trên private leaderboard, đó là sử dụng RoBerta. Tuy nhiên, với trình độ hiện tại của các bạn trong nhóm thì phương pháp này đang vượt quá tầm với vì vậy phương hướng tiếp cận của nhóm đã vẽ ra như sau:\n",
        "\n",
        "- Nhóm tiến hành giải bài toán theo cách riêng của nhóm: Học thống kê + học kết hợp deep learning \\\n",
        "- Sau khi ra kết quả training, nếu chưa đạt được kết quả nằm trong top 20 của private leaderboard, thì nhóm sẽ tiếp tục tiếp cận dần các phương pháp Deep learning \\\n",
        "- Sau cùng, nếu vẫn chưa đạt được kết quả khả quan với những kiến thức deep learning đã học, nhóm sẽ tiến hành giải tiếp bài toán theo phương pháp của hạng 3 (lúc này các thành viên đã đủ kiến thức)\n",
        "\n",
        "Hiện đang tham khảo: https://medium.com/analytics-vidhya/tweet-sentiment-extraction-e5d242ff93c2 (phần QuestionAnswering)\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "buOs0j5cul30"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Một số paper về Deep learning nhóm đã tham khảo:\n",
        "- LSTM: https://arxiv.org/pdf/1503.04069.pdf\n",
        "- Attention +  Transformer: https://arxiv.org/pdf/1706.03762.pdf\n",
        "- BERT: https://arxiv.org/pdf/1810.04805.pdf\n",
        "- Cách sử dụng: thông qua thư viện Tensorflow, Transformer: https://huggingface.co/docs/transformers/quicktour\n"
      ],
      "metadata": {
        "id": "hqZX5XEfjjqS"
      },
      "id": "hqZX5XEfjjqS"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1Yifq6l49Ik",
        "outputId": "616791fc-ab5c-4f68-f71c-dc76dc1f3481"
      },
      "id": "-1Yifq6l49Ik",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Sq7RjgxK20"
      },
      "source": [
        "# II. Giải quyết bài toán và cài đặt:"
      ],
      "id": "k4Sq7RjgxK20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_ulm98oBHzT"
      },
      "source": [
        "# 1. Học dựa trên thống kê\n",
        "\n",
        "- Tiền xử lý và khám phá dữ liệu\n",
        "- Xây dựng từ điển tần suất theo sentiment\n",
        "- Xây dựng Bag-of-word theo xác suất Naive-Bayes\n",
        "- Trích text bằng likelihood của bước trên và ước lượng ngưỡng\n",
        "- Ánh xạ text đã rút trích về nguyên mẫu của từng từ trong text (đề yêu cầu)\n",
        "- Đánh giá bằng độ đo Jaccard Similarity\n",
        "- Tunning trên tập val để tìm siêu tham số tốt nhất\n",
        "- Dự đoán rút trích trên tập test\n",
        " "
      ],
      "id": "N_ulm98oBHzT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Kết quả: 0.6249 trên train, 0.6137 trên val"
      ],
      "metadata": {
        "id": "ZsmZkW3GgA3l"
      },
      "id": "ZsmZkW3GgA3l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvBSxcg5B1Jv"
      },
      "source": [
        "### 1.1 Lấy dữ liệu, Khám phá và tiền xử lý:"
      ],
      "id": "lvBSxcg5B1Jv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Weea1SUCBmz"
      },
      "source": [
        "#### 1.1.1 Thư viện:"
      ],
      "id": "8Weea1SUCBmz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0c2dbd1",
        "outputId": "70890861-6fca-480b-e1e3-ec643ff8af74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Applied-Data-Science-Group-7'...\n",
            "remote: Enumerating objects: 141, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
            "remote: Total 141 (delta 68), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (141/141), 5.33 MiB | 4.55 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hoanglam372000/Applied-Data-Science-Group-7.git"
      ],
      "id": "b0c2dbd1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eff8fbcf",
        "outputId": "920e1b01-7def-4d9e-b288-8b69050632ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 30 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 61 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 92 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 170 kB 4.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=43410009f92a5983dea5e878cd9f40d04bd54042180f74f8be1ffc71dd442518\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "!pip install pyenchant\n",
        "#!pip install pycontractions"
      ],
      "id": "eff8fbcf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b92498d",
        "outputId": "b631d6b4-9a0e-44ce-e3c5-adda9d4399ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import emoji\n",
        "#import contractions\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#print(stopwords.words('english'))\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('names')\n",
        "names = nltk.corpus.names.words('male.txt')+nltk.corpus.names.words('female.txt')\n",
        "names_lower = [name.lower() for name in names]"
      ],
      "id": "9b92498d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK7e9Cf_q-AD",
        "outputId": "022386c3-ab02-480b-bd42-c223cafd4158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting afinn\n",
            "  Downloading afinn-0.1.tar.gz (52 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▎                         | 10 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 52 kB 1.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53448 sha256=305a88c127b32b7b0503ce14ca64386bda8ba9d5cf04735d83942b9d0f7815a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/16/3a/9f0953027434eab5dadf3f33ab3298fa95afa8292fcf7aba75\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install afinn"
      ],
      "id": "sK7e9Cf_q-AD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2inlf1hrDkj"
      },
      "outputs": [],
      "source": [
        "from afinn import Afinn\n",
        "afinn = Afinn(language='en')"
      ],
      "id": "m2inlf1hrDkj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLIRG_m1CdvD"
      },
      "source": [
        "#### 1.1.2 Khám phá dữ liệu"
      ],
      "id": "qLIRG_m1CdvD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cae6a810"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Applied-Data-Science-Group-7/train.csv')"
      ],
      "id": "cae6a810"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "004d834b",
        "outputId": "1f9e1a24-b31c-41c6-cbe1-d4996ced0c3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27476</th>\n",
              "      <td>4eac33d1c0</td>\n",
              "      <td>wish we could come see u on Denver  husband lost his job and can`t afford it</td>\n",
              "      <td>d lost</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27477</th>\n",
              "      <td>4f4c4fc327</td>\n",
              "      <td>I`ve wondered about rake to.  The client has made it clear .NET only, don`t force devs to learn a new lang  #agile #ccnet</td>\n",
              "      <td>, don`t force</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27478</th>\n",
              "      <td>f67aae2310</td>\n",
              "      <td>Yay good for both of you. Enjoy the break - you probably need it after such hectic weekend  Take care hun xxxx</td>\n",
              "      <td>Yay good for both of you.</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27479</th>\n",
              "      <td>ed167662a5</td>\n",
              "      <td>But it was worth it  ****.</td>\n",
              "      <td>But it was worth it  ****.</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27480</th>\n",
              "      <td>6f7127d9d7</td>\n",
              "      <td>All this flirting going on - The ATG smiles. Yay.  ((hugs))</td>\n",
              "      <td>All this flirting going on - The ATG smiles. Yay.  ((hugs)</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27481 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           textID  ... sentiment\n",
              "0      cb774db0d1  ...   neutral\n",
              "1      549e992a42  ...  negative\n",
              "2      088c60f138  ...  negative\n",
              "3      9642c003ef  ...  negative\n",
              "4      358bd9e861  ...  negative\n",
              "...           ...  ...       ...\n",
              "27476  4eac33d1c0  ...  negative\n",
              "27477  4f4c4fc327  ...  negative\n",
              "27478  f67aae2310  ...  positive\n",
              "27479  ed167662a5  ...  positive\n",
              "27480  6f7127d9d7  ...   neutral\n",
              "\n",
              "[27481 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df"
      ],
      "id": "004d834b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33751737",
        "outputId": "e8ba3938-dbd6-47af-f997-a4e5dfc0a1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27481 entries, 0 to 27480\n",
            "Data columns (total 4 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   textID         27481 non-null  object\n",
            " 1   text           27480 non-null  object\n",
            " 2   selected_text  27480 non-null  object\n",
            " 3   sentiment      27481 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 858.9+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ],
      "id": "33751737"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35c68f84"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace = True)"
      ],
      "id": "35c68f84"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6639f1f6",
        "outputId": "81f2ac4b-5392-4ef0-c66c-ff7f2255b1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 27480 entries, 0 to 27480\n",
            "Data columns (total 4 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   textID         27480 non-null  object\n",
            " 1   text           27480 non-null  object\n",
            " 2   selected_text  27480 non-null  object\n",
            " 3   sentiment      27480 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 1.0+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ],
      "id": "6639f1f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ca4fc3"
      },
      "source": [
        "#### 1.1.3 Tách tập dữ liệu"
      ],
      "id": "b6ca4fc3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75d8621d"
      },
      "outputs": [],
      "source": [
        "# Tách tập huấn luyện và tập validation theo tỉ lệ 70%:30%\n",
        "X_df = df.drop(labels = 'selected_text',axis = 1)\n",
        "y_ex = df.selected_text\n",
        "y_class = df.sentiment\n",
        "train_X_df, val_X_df, train_y_ex, val_y_ex = train_test_split(X_df, y_ex, test_size=0.3, \n",
        "                                                              stratify=y_class, random_state=0)"
      ],
      "id": "75d8621d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3af4df9",
        "outputId": "d3c1ae06-3af8-418d-bfde-576999218aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X:  (19236, 3)\n",
            "train_y:  (19236,)\n",
            "val_X:  (8244, 3)\n",
            "val_y:  (8244,)\n"
          ]
        }
      ],
      "source": [
        "print('train_X: ',train_X_df.shape)\n",
        "print('train_y: ',train_y_ex.shape)\n",
        "print('val_X: ',val_X_df.shape)\n",
        "print('val_y: ',val_y_ex.shape)\n"
      ],
      "id": "e3af4df9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b05a364e"
      },
      "source": [
        "### 1.2 Tiền xử lý + Khám phá tập train"
      ],
      "id": "b05a364e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awyk6UXcC5gA"
      },
      "source": [
        "#### 1.2.1 Khám phá tập train"
      ],
      "id": "awyk6UXcC5gA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5c6f4b6",
        "outputId": "67262b3b-44a8-4f8b-a0cd-4b064d645853"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Phân bố của các lớp:\n",
        "train_X_df.sentiment.value_counts(normalize= True).plot(kind=\"pie\",labels = None, legend='auto', figsize=(8,6), autopct=\"%.1f%%\");"
      ],
      "id": "b5c6f4b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8e5df71",
        "outputId": "512449bb-c9df-477a-e51e-76d3079b77f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 19236 entries, 652 to 12238\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   textID     19236 non-null  object\n",
            " 1   text       19236 non-null  object\n",
            " 2   sentiment  19236 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 601.1+ KB\n"
          ]
        }
      ],
      "source": [
        "train_X_df.info()"
      ],
      "id": "d8e5df71"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64ae1685",
        "outputId": "39c6eb83-fd32-4812-9b91-f3b671586ff6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train_X_df.duplicated().sum()"
      ],
      "id": "64ae1685"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76b2e318",
        "outputId": "52a1ff78-62b9-4aaf-ac1d-fa80e9f4798b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "train_y_ex.isnull().sum()"
      ],
      "id": "76b2e318"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee0725cc"
      },
      "source": [
        "#### 1.2.2 Tiền xử lý văn bản"
      ],
      "id": "ee0725cc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TifZy-OvUEbO"
      },
      "outputs": [],
      "source": [
        " #nltk.pos_tag(['i','am','over','the','moon'])"
      ],
      "id": "TifZy-OvUEbO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "025BPLnKOV3N"
      },
      "outputs": [],
      "source": [
        "# import enchant\n",
        "# from nltk.metrics import edit_distance\n",
        "# class SpellingReplacer(object):\n",
        "#     def __init__(self, dict_name='en', max_dist=2):\n",
        "#         self.spell_dict = enchant.Dict(dict_name)\n",
        "#         self.max_dist = max_dist\n",
        "#     def replace(self, word):\n",
        "#         if self.spell_dict.check(word):\n",
        "#             return word\n",
        "#         suggestions = self.spell_dict.suggest(word)\n",
        "#         if suggestions and edit_distance(word, suggestions[0]) <=self.max_dist:\n",
        "#             return suggestions[0]\n",
        "#         else:\n",
        "#             return word\n",
        "# replacer = SpellingReplacer()\n",
        "# replacer.replace('cookbok')"
      ],
      "id": "025BPLnKOV3N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cceeb36"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "# class xử lý vấn đề trùng ký tự trong từ, ví dụ yoooouuuu -> you, nooooo -> no.\n",
        "#sử dụng pattern của regular expression, tuy nhiên phải quét thêm qua wordnet để tránh trường hợp như từ goose bị chuyển thành gose (mặc dù goose là từ đúng)\n",
        "class RepeatReplacer(object):\n",
        "    def __init__(self):\n",
        "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "        self.repl = r'\\1\\2\\3'\n",
        "    def replace(self, word):\n",
        "        if wordnet.synsets(word):\n",
        "            return word\n",
        "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
        "        if repl_word != word:\n",
        "            return self.replace(repl_word)\n",
        "        else:\n",
        "            return repl_word\n",
        "char_repeat_correct = RepeatReplacer()\n",
        "def preprocessing_word(word):\n",
        "  \n",
        "  if word != emoji.demojize(word):\n",
        "    return emoji.demojize(word)\n",
        "  #word = contractions.fix(word)\n",
        "  #word = stemmer.stem(word)\n",
        "  \n",
        "  temp=[ \"html_tag\",'URL','HIDDEN','tag',\"hash_tag\"]\n",
        "  if word in temp:\n",
        "    return word\n",
        "  #số theo từ\n",
        "  if word.isdigit():  \n",
        "    return 'NUMBER'\n",
        "  try: \n",
        "    float(word)\n",
        "    return 'NUMBER'\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "  #word=word.lower() # viết thườngthường\n",
        "  #spell_corrector = Speller(lang='en') #sửa chính tả theo từng từ \n",
        "  #word=spell_corrector(word)\n",
        "\n",
        "  # rút gọn theo từ\n",
        "  #lemmatizer = WordNetLemmatizer()\n",
        "  #wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "  #pos_tagged = nltk.pos_tag(word.split())\n",
        "  \n",
        "  #word = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged]\n",
        "  #word = ' '.join(word)\n",
        "\n",
        "  #word=word.translate(str.maketrans('', '', string.punctuation)) # chấm câu theo từ (vì tránh emoij)\n",
        "  word = re.sub(pattern=r'[\"!\\.#$%&\\'()*+-/:;<=>@\\?\\[\\]^_`{|}~0-9ï¿½]+', repl='', string=word)\n",
        "  word = char_repeat_correct.replace(word)\n",
        "  #word = stemmer.stem(word)\n",
        "  #if (word in  names):\n",
        "   # return '<Name>'\n",
        "  #word = stemmer.stem(word)\n",
        "  word = word.lower()\n",
        "  \n",
        "  if word==''or word ==' ':\n",
        "    return \"Special\"\n",
        "  \n",
        "  # if 'not' not in word and len(word.split())!=1:\n",
        "  #   return word.split()[0]+'_'\n",
        "  return word.replace(' ','_')\n",
        "\n",
        "def preprocessing_text(tweet,sentiment):\n",
        "   raw_text = str(tweet).strip().replace('\\n', ' ')#.lower()\n",
        "   # emoij\n",
        "   raw_text=emoji.demojize(raw_text)\n",
        "   e_r=r':\\w+_\\w+:'\n",
        "   emoij_list=re.findall(e_r,raw_text)\n",
        "   for e in emoij_list:\n",
        "     raw_text=raw_text.replace(e,' ' + emoji.emojize(e) + ' ') # \"hi❤️👍\"\n",
        "   if sentiment !=0: \n",
        "     raw_text = re.sub(pattern=r'(?<=\\w)(\\.){2,}?(?=\\w)', repl=' ', string=raw_text)\n",
        "   new_tweet = raw_text.split()\n",
        "   len_=len(new_tweet)\n",
        "   index2word = { w:new_tweet[w] for w in range(len_)}\n",
        "   #parser = BeautifulSoup(raw_text, \"html.parser\")\n",
        "   #raw_text = parser.get_text(separator = \"<html_tag>\")\n",
        "   raw_text = re.sub(pattern=r'https?://\\S+|www\\.\\S+', repl='URL', string=raw_text)\n",
        "   raw_text = re.sub(pattern='\\*{3,}?', repl='HIDDEN', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'@\\w+', repl='tag', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'#\\w+', repl='hash_tag', string=raw_text)\n",
        "   \n",
        "   word_list=raw_text.split()\n",
        "   return index2word,dict(zip([*range(len_)],map(preprocessing_word,word_list)))"
      ],
      "id": "2cceeb36"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cad376e3",
        "outputId": "8d38cbf3-1dfc-456f-e54e-8c720e7b1bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index2word:  {0: 'nooooooooo'}\n",
            "preprocessed_tweet:  {0: 'no'}\n"
          ]
        }
      ],
      "source": [
        "# Test:\n",
        "tweet = ' My mom just texted.......me and told *ship* James that Rodney3 charlie wasï¿½ *** chasing?? and fireflies... in their :)) backyard. Awwwww I`m miss him!!! @hoanglam 123 ://    http://twitpic.com/66pn1'\n",
        "tweet2 = 'nooooooooo'\n",
        "index2word_temp,preprocessed_tweet_temp = preprocessing_text(tweet2,1)\n",
        "print('index2word: ',index2word_temp)\n",
        "print('preprocessed_tweet: ',preprocessed_tweet_temp)"
      ],
      "id": "cad376e3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii4Zq1Ik8GIf",
        "outputId": "21d8236a-5c02-40c3-97ea-74fa5d467dca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "':))))))'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "char_repeat_correct.replace(':))))))')"
      ],
      "id": "ii4Zq1Ik8GIf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49c6777d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_tweets(X_df):\n",
        "  X_df = X_df.copy()\n",
        "  raw_tweets = X_df.text.to_list()\n",
        "  X_df.sentiment = X_df.sentiment.map({'negative':-1,'neutral':0,'positive':1})\n",
        "  index2word_tweets = [] #list of dictionaries\n",
        "  preprocessed_tweets = [] #list of dictionaries\n",
        "  cleaned_texts = []\n",
        "  for tweet,senti in zip(raw_tweets,X_df.sentiment.to_list()):\n",
        "    index2word, preprocessed = preprocessing_text(tweet,senti)\n",
        "    index2word_tweets.append(index2word)\n",
        "    preprocessed_tweets.append(preprocessed)\n",
        "    cleaned_texts.append(' '.join(list(preprocessed.values())))\n",
        "  preprocessed_X_df = pd.DataFrame(data={'textID':X_df.textID,'index2word':index2word_tweets, 'preprocessed_texts':preprocessed_tweets,'sentiment':X_df.sentiment})\n",
        "  cleaned_int2text_df = pd.DataFrame(data={'textID':X_df.textID,'Raw_text':X_df.text, 'Cleaned_texts':cleaned_texts,'sentiment':X_df.sentiment})\n",
        "  return preprocessed_X_df,cleaned_int2text_df\n",
        "\n",
        "def _preprocessed_y_ex(X_df,y_ex):\n",
        "  preprocessed_y_ex = []\n",
        "  X_df = X_df.copy()\n",
        "  cleaned_texts = []\n",
        "  for tweet,senti in zip(y_ex.to_list(),X_df.sentiment.to_list()):\n",
        "      index2word, preprocessed_y = preprocessing_text(tweet,senti)\n",
        "      preprocessed_y_ex.append(preprocessed_y)\n",
        "      cleaned_texts.append(' '.join(list(preprocessed_y.values())))\n",
        "\n",
        "  X_df['sentiment'] = X_df.sentiment.map({'negative':-1,'neutral':0,'positive':1}).to_list()\n",
        "  return pd.DataFrame(data = {'preprocessed_texts': preprocessed_y_ex,'Cleaned_text': cleaned_texts,'sentiment':X_df.sentiment})\n",
        "\n"
      ],
      "id": "49c6777d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPa_6PkUHBJ-",
        "outputId": "68056e21-429b-42b0-a649-e6cc5392df4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>a834cef9c6</td>\n",
              "      <td>1st presentation for senior boards? FML</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24883</th>\n",
              "      <td>5bfad683d5</td>\n",
              "      <td>yummy curry  save me some please lol</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15385</th>\n",
              "      <td>08a8d660b1</td>\n",
              "      <td>I wish I knew someone down there who could hook us up.   a friend told me once that they got crappy seats for a show in CHI</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16753</th>\n",
              "      <td>99a0ff3bef</td>\n",
              "      <td>Lucky u Im stuck in the rain...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>40e7becabf</td>\n",
              "      <td>Hes just not that into you</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           textID  ... sentiment\n",
              "652    a834cef9c6  ...  negative\n",
              "24883  5bfad683d5  ...  positive\n",
              "15385  08a8d660b1  ...  negative\n",
              "16753  99a0ff3bef  ...   neutral\n",
              "19     40e7becabf  ...   neutral\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train_X_df.head(5)"
      ],
      "id": "xPa_6PkUHBJ-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv5SE8NKFOPc"
      },
      "outputs": [],
      "source": [
        "#_preprocessed_y_ex(train_X_df.head(100),train_y_ex.head(100))"
      ],
      "id": "Vv5SE8NKFOPc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6ik6GL6Te_F"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "z6ik6GL6Te_F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQQ1nPTw_X_7",
        "outputId": "4ca8082f-33a0-4428-83e3-25e73b39317d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>index2word</th>\n",
              "      <th>preprocessed_texts</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>a834cef9c6</td>\n",
              "      <td>{0: '1st', 1: 'presentation', 2: 'for', 3: 'senior', 4: 'boards?', 5: 'FML'}</td>\n",
              "      <td>{0: 'st', 1: 'presentation', 2: 'for', 3: 'senior', 4: 'boards', 5: 'fml'}</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24883</th>\n",
              "      <td>5bfad683d5</td>\n",
              "      <td>{0: 'yummy', 1: 'curry', 2: 'save', 3: 'me', 4: 'some', 5: 'please', 6: 'lol'}</td>\n",
              "      <td>{0: 'yummy', 1: 'curry', 2: 'save', 3: 'me', 4: 'some', 5: 'please', 6: 'lol'}</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15385</th>\n",
              "      <td>08a8d660b1</td>\n",
              "      <td>{0: 'I', 1: 'wish', 2: 'I', 3: 'knew', 4: 'someone', 5: 'down', 6: 'there', 7: 'who', 8: 'could', 9: 'hook', 10: 'us', 11: 'up.', 12: 'a', 13: 'friend', 14: 'told', 15: 'me', 16: 'once', 17: 'that...</td>\n",
              "      <td>{0: 'i', 1: 'wish', 2: 'i', 3: 'knew', 4: 'someone', 5: 'down', 6: 'there', 7: 'who', 8: 'could', 9: 'hook', 10: 'us', 11: 'up', 12: 'a', 13: 'friend', 14: 'told', 15: 'me', 16: 'once', 17: 'that'...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16753</th>\n",
              "      <td>99a0ff3bef</td>\n",
              "      <td>{0: 'Lucky', 1: 'u', 2: 'Im', 3: 'stuck', 4: 'in', 5: 'the', 6: 'rain...'}</td>\n",
              "      <td>{0: 'lucky', 1: 'u', 2: 'im', 3: 'stuck', 4: 'in', 5: 'the', 6: 'rain'}</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>40e7becabf</td>\n",
              "      <td>{0: 'Hes', 1: 'just', 2: 'not', 3: 'that', 4: 'into', 5: 'you'}</td>\n",
              "      <td>{0: 'hes', 1: 'just', 2: 'not', 3: 'that', 4: 'into', 5: 'you'}</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16799</th>\n",
              "      <td>82d211bdd5</td>\n",
              "      <td>{0: 'i', 1: 'think', 2: 'i', 3: 'hate', 4: 'you.', 5: 'i', 6: 'didnt', 7: 'really', 8: 'want', 9: 'to', 10: 'but', 11: 'you', 12: 'make', 13: 'it', 14: 'hard', 15: 'for', 16: 'me', 17: 'to', 18: '...</td>\n",
              "      <td>{0: 'i', 1: 'think', 2: 'i', 3: 'hate', 4: 'you', 5: 'i', 6: 'didnt', 7: 'really', 8: 'want', 9: 'to', 10: 'but', 11: 'you', 12: 'make', 13: 'it', 14: 'hard', 15: 'for', 16: 'me', 17: 'to', 18: 'l...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6706</th>\n",
              "      <td>7a775cc933</td>\n",
              "      <td>{0: 'If', 1: 'I', 2: 'were', 3: 'a', 4: 'transformer...', 5: 'I`d', 6: 'step', 7: 'on', 8: 'people', 9: 'just', 10: 'to', 11: 'hear', 12: 'them', 13: '*squoosh*.', 14: 'But', 15: 'I`m', 16: 'not',...</td>\n",
              "      <td>{0: 'if', 1: 'i', 2: 'were', 3: 'a', 4: 'transformer', 5: 'id', 6: 'step', 7: 'on', 8: 'people', 9: 'just', 10: 'to', 11: 'hear', 12: 'them', 13: 'squosh', 14: 'but', 15: 'im', 16: 'not', 17: 'so'...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1967</th>\n",
              "      <td>127ac4d1d2</td>\n",
              "      <td>{0: 'ohay', 1: 'clean', 2: 'teeth'}</td>\n",
              "      <td>{0: 'ohay', 1: 'clean', 2: 'teeth'}</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9181</th>\n",
              "      <td>4b8cb4449b</td>\n",
              "      <td>{0: 'Im', 1: 'fine', 2: 'also..up', 3: 'way', 4: 'to', 5: 'early..lol...soo', 6: 'those', 7: 'r', 8: 'all', 9: 'ur', 10: 'dogs?', 11: 'to', 12: 'kut3', 13: '..i', 14: 'def', 15: 'want', 16: 'a', 1...</td>\n",
              "      <td>{0: 'im', 1: 'fine', 2: 'alsoup', 3: 'way', 4: 'to', 5: 'earlylolso', 6: 'those', 7: 'r', 8: 'all', 9: 'ur', 10: 'dogs', 11: 'to', 12: 'kut', 13: 'i', 14: 'def', 15: 'want', 16: 'a', 17: 'pit', 18...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17204</th>\n",
              "      <td>c06c7c3adf</td>\n",
              "      <td>{0: 'How', 1: 'about', 2: 'give', 3: 'golf', 4: 'lessons??', 5: 'AND', 6: 'sing', 7: 'to', 8: 'your', 9: 'student', 10: 'taking', 11: 'lessons??', 12: '(JAKEOWEN2009', 13: 'live', 14: '&gt;', 15: 'ht...</td>\n",
              "      <td>{0: 'how', 1: 'about', 2: 'give', 3: 'golf', 4: 'lessons', 5: 'and', 6: 'sing', 7: 'to', 8: 'your', 9: 'student', 10: 'taking', 11: 'lessons', 12: 'jakeowen', 13: 'live', 14: 'Special', 15: 'URL'}</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           textID  ... sentiment\n",
              "652    a834cef9c6  ...        -1\n",
              "24883  5bfad683d5  ...         1\n",
              "15385  08a8d660b1  ...        -1\n",
              "16753  99a0ff3bef  ...         0\n",
              "19     40e7becabf  ...         0\n",
              "...           ...  ...       ...\n",
              "16799  82d211bdd5  ...        -1\n",
              "6706   7a775cc933  ...         0\n",
              "1967   127ac4d1d2  ...         1\n",
              "9181   4b8cb4449b  ...         0\n",
              "17204  c06c7c3adf  ...         0\n",
              "\n",
              "[100 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Test:\n",
        "preprocessed_X_df_temp,cleaned_int2text_df_temp = preprocess_tweets(train_X_df.head(100))\n",
        "preprocessed_X_df_temp"
      ],
      "id": "JQQ1nPTw_X_7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XROLAww9DetK"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "XROLAww9DetK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbNc5z3IDZt1"
      },
      "source": [
        "### 2. Mô hình hoá dữ liệu:"
      ],
      "id": "YbNc5z3IDZt1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mui8N94wDFm"
      },
      "source": [
        "#### 2.1 Tiền xử lý:"
      ],
      "id": "2Mui8N94wDFm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4uXbxBij9Je"
      },
      "outputs": [],
      "source": [
        "preprocessed_train_X, cleaned_train_X = preprocess_tweets(train_X_df)\n",
        "preprocessed_train_y_ex = _preprocessed_y_ex(train_X_df,train_y_ex)\n",
        "preprocessed_val_X, cleaned_val_X = preprocess_tweets(val_X_df)\n",
        "preprocessed_val_y_ex = _preprocessed_y_ex(val_X_df,val_y_ex)"
      ],
      "id": "g4uXbxBij9Je"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnvLrVHRGtmV"
      },
      "outputs": [],
      "source": [
        "#cleaned_train_X"
      ],
      "id": "AnvLrVHRGtmV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfxKU98rtfkD",
        "outputId": "f679e819-55d9-4db5-c61d-1fa2b5329388"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>{0: 'st', 1: 'presentation', 2: 'for', 3: 'senior', 4: 'boards', 5: 'fml'}</td>\n",
              "      <td>{0: 'fml'}</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24883</th>\n",
              "      <td>{0: 'yummy', 1: 'curry', 2: 'save', 3: 'me', 4: 'some', 5: 'please', 6: 'lol'}</td>\n",
              "      <td>{0: 'yummy'}</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15385</th>\n",
              "      <td>{0: 'i', 1: 'wish', 2: 'i', 3: 'knew', 4: 'someone', 5: 'down', 6: 'there', 7: 'who', 8: 'could', 9: 'hook', 10: 'us', 11: 'up', 12: 'a', 13: 'friend', 14: 'told', 15: 'me', 16: 'once', 17: 'that'...</td>\n",
              "      <td>{0: 'p', 1: 'a', 2: 'friend', 3: 'told', 4: 'me', 5: 'once', 6: 'that', 7: 'they', 8: 'got', 9: 'crappy', 10: 'seats', 11: 'for', 12: 'a', 13: 'show', 14: 'in'}</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16753</th>\n",
              "      <td>{0: 'lucky', 1: 'u', 2: 'im', 3: 'stuck', 4: 'in', 5: 'the', 6: 'rain'}</td>\n",
              "      <td>{0: 'lucky', 1: 'u', 2: 'im', 3: 'stuck', 4: 'in', 5: 'the', 6: 'rain'}</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>{0: 'hes', 1: 'just', 2: 'not', 3: 'that', 4: 'into', 5: 'you'}</td>\n",
              "      <td>{0: 'hes', 1: 'just', 2: 'not', 3: 'that', 4: 'into', 5: 'you'}</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23126</th>\n",
              "      <td>{0: 'gah', 1: 'Special', 2: 'this', 3: 'weather', 4: 'sucksss', 5: 'Special'}</td>\n",
              "      <td>{0: 'gah', 1: 'Special', 2: 'this', 3: 'weather', 4: 'sucksss', 5: 'Special'}</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27051</th>\n",
              "      <td>{0: 'apologies', 1: 'dont', 2: 'fix', 3: 'hurt', 4: 'feelings', 5: 'kev', 6: 'anyway', 7: 'ill', 8: 'get', 9: 'it', 10: 'from', 11: 'u', 12: 'next', 13: 'time', 14: 'i', 15: 'c', 16: 'u'}</td>\n",
              "      <td>{0: 'apologies', 1: 'dont', 2: 'fix', 3: 'hurt', 4: 'feelings', 5: 'kev'}</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11314</th>\n",
              "      <td>{0: 'Special', 1: 'follow', 2: 'my', 3: 'girl', 4: 'meagan'}</td>\n",
              "      <td>{0: 'Special', 1: 'follow', 2: 'my', 3: 'girl', 4: 'meagan'}</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5854</th>\n",
              "      <td>{0: 'this', 1: 'is', 2: 'really', 3: 'helpful', 4: 'you', 5: 'swear', 6: 'a', 7: 'lot', 8: 'i', 9: 'just', 10: 'realized', 11: 'haha'}</td>\n",
              "      <td>{0: 'helpful'}</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8160</th>\n",
              "      <td>{0: 'happy', 1: 'mothers', 2: 'day', 3: 'x', 4: 'mum', 5: 'i', 6: 'love', 7: 'you'}</td>\n",
              "      <td>{0: 'happy'}</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                          text  ... sentiment\n",
              "652                                                                                                                                 {0: 'st', 1: 'presentation', 2: 'for', 3: 'senior', 4: 'boards', 5: 'fml'}  ...        -1\n",
              "24883                                                                                                                           {0: 'yummy', 1: 'curry', 2: 'save', 3: 'me', 4: 'some', 5: 'please', 6: 'lol'}  ...         1\n",
              "15385  {0: 'i', 1: 'wish', 2: 'i', 3: 'knew', 4: 'someone', 5: 'down', 6: 'there', 7: 'who', 8: 'could', 9: 'hook', 10: 'us', 11: 'up', 12: 'a', 13: 'friend', 14: 'told', 15: 'me', 16: 'once', 17: 'that'...  ...        -1\n",
              "16753                                                                                                                                  {0: 'lucky', 1: 'u', 2: 'im', 3: 'stuck', 4: 'in', 5: 'the', 6: 'rain'}  ...         0\n",
              "19                                                                                                                                             {0: 'hes', 1: 'just', 2: 'not', 3: 'that', 4: 'into', 5: 'you'}  ...         0\n",
              "23126                                                                                                                            {0: 'gah', 1: 'Special', 2: 'this', 3: 'weather', 4: 'sucksss', 5: 'Special'}  ...        -1\n",
              "27051              {0: 'apologies', 1: 'dont', 2: 'fix', 3: 'hurt', 4: 'feelings', 5: 'kev', 6: 'anyway', 7: 'ill', 8: 'get', 9: 'it', 10: 'from', 11: 'u', 12: 'next', 13: 'time', 14: 'i', 15: 'c', 16: 'u'}  ...        -1\n",
              "11314                                                                                                                                             {0: 'Special', 1: 'follow', 2: 'my', 3: 'girl', 4: 'meagan'}  ...         0\n",
              "5854                                                                    {0: 'this', 1: 'is', 2: 'really', 3: 'helpful', 4: 'you', 5: 'swear', 6: 'a', 7: 'lot', 8: 'i', 9: 'just', 10: 'realized', 11: 'haha'}  ...         1\n",
              "8160                                                                                                                       {0: 'happy', 1: 'mothers', 2: 'day', 3: 'x', 4: 'mum', 5: 'i', 6: 'love', 7: 'you'}  ...         1\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "#pd.set_option('display.max_rows', df.shape[0]+1)\n",
        "visual_df = pd.DataFrame(data ={'text':preprocessed_train_X.preprocessed_texts,'selected_text':preprocessed_train_y_ex.preprocessed_texts,'sentiment':preprocessed_train_X.sentiment})\n",
        "visual_df.head(10)"
      ],
      "id": "sfxKU98rtfkD"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_train_y_ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jx-2fwz9zAQ",
        "outputId": "27c3d6af-8bb9-4b49-a2f5-696cac720f72"
      },
      "id": "_Jx-2fwz9zAQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessed_texts</th>\n",
              "      <th>Cleaned_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>{0: 'fml'}</td>\n",
              "      <td>fml</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24883</th>\n",
              "      <td>{0: 'yummy'}</td>\n",
              "      <td>yummy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15385</th>\n",
              "      <td>{0: 'p', 1: 'a', 2: 'friend', 3: 'told', 4: 'me', 5: 'once', 6: 'that', 7: 'they', 8: 'got', 9: 'crappy', 10: 'seats', 11: 'for', 12: 'a', 13: 'show', 14: 'in'}</td>\n",
              "      <td>p a friend told me once that they got crappy seats for a show in</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16753</th>\n",
              "      <td>{0: 'lucky', 1: 'u', 2: 'im', 3: 'stuck', 4: 'in', 5: 'the', 6: 'rain'}</td>\n",
              "      <td>lucky u im stuck in the rain</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>{0: 'hes', 1: 'just', 2: 'not', 3: 'that', 4: 'into', 5: 'you'}</td>\n",
              "      <td>hes just not that into you</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21628</th>\n",
              "      <td>{0: 'best'}</td>\n",
              "      <td>best</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24747</th>\n",
              "      <td>{0: 'yeah', 1: 'hes', 2: 'also', 3: 'partial', 4: 'to', 5: 'jt', 6: 'and', 7: 'ti', 8: 'dead', 9: 'and', 10: 'gone', 11: 'and', 12: 'pinks', 13: 'sobre'}</td>\n",
              "      <td>yeah hes also partial to jt and ti dead and gone and pinks sobre</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18164</th>\n",
              "      <td>{0: 'oui', 1: 'i', 2: 'figured', 3: 'that', 4: 'since', 5: 'its', 6: 'a', 7: 'day', 8: 'off', 9: 'id', 10: 'better', 11: 'start', 12: 'brushing', 13: 'up', 14: 'on', 15: 'my', 16: 'french', 17: 's...</td>\n",
              "      <td>oui i figured that since its a day off id better start brushing up on my french so far i have learnt bonjour and oui</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10041</th>\n",
              "      <td>{0: 'if', 1: 'i', 2: 'can', 3: 'get', 4: 'a', 5: 'ticket', 6: 'but', 7: 'the', 8: 'pickings', 9: 'are', 10: 'lokin', 11: 'slim', 12: 'so', 13: 'prob', 14: 'not'}</td>\n",
              "      <td>if i can get a ticket but the pickings are lokin slim so prob not</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12238</th>\n",
              "      <td>{0: 'thank', 1: 'you', 2: 'make', 3: 'sure', 4: 'u', 5: 'read', 6: 'the', 7: 'groups', 8: 'description', 9: 'to', 10: 'better', 11: 'understand', 12: 'this', 13: 'project'}</td>\n",
              "      <td>thank you make sure u read the groups description to better understand this project</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19236 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                            preprocessed_texts  ... sentiment\n",
              "652                                                                                                                                                                                                 {0: 'fml'}  ...        -1\n",
              "24883                                                                                                                                                                                             {0: 'yummy'}  ...         1\n",
              "15385                                         {0: 'p', 1: 'a', 2: 'friend', 3: 'told', 4: 'me', 5: 'once', 6: 'that', 7: 'they', 8: 'got', 9: 'crappy', 10: 'seats', 11: 'for', 12: 'a', 13: 'show', 14: 'in'}  ...        -1\n",
              "16753                                                                                                                                  {0: 'lucky', 1: 'u', 2: 'im', 3: 'stuck', 4: 'in', 5: 'the', 6: 'rain'}  ...         0\n",
              "19                                                                                                                                             {0: 'hes', 1: 'just', 2: 'not', 3: 'that', 4: 'into', 5: 'you'}  ...         0\n",
              "...                                                                                                                                                                                                        ...  ...       ...\n",
              "21628                                                                                                                                                                                              {0: 'best'}  ...         1\n",
              "24747                                                {0: 'yeah', 1: 'hes', 2: 'also', 3: 'partial', 4: 'to', 5: 'jt', 6: 'and', 7: 'ti', 8: 'dead', 9: 'and', 10: 'gone', 11: 'and', 12: 'pinks', 13: 'sobre'}  ...         0\n",
              "18164  {0: 'oui', 1: 'i', 2: 'figured', 3: 'that', 4: 'since', 5: 'its', 6: 'a', 7: 'day', 8: 'off', 9: 'id', 10: 'better', 11: 'start', 12: 'brushing', 13: 'up', 14: 'on', 15: 'my', 16: 'french', 17: 's...  ...         0\n",
              "10041                                        {0: 'if', 1: 'i', 2: 'can', 3: 'get', 4: 'a', 5: 'ticket', 6: 'but', 7: 'the', 8: 'pickings', 9: 'are', 10: 'lokin', 11: 'slim', 12: 'so', 13: 'prob', 14: 'not'}  ...         0\n",
              "12238                             {0: 'thank', 1: 'you', 2: 'make', 3: 'sure', 4: 'u', 5: 'read', 6: 'the', 7: 'groups', 8: 'description', 9: 'to', 10: 'better', 11: 'understand', 12: 'this', 13: 'project'}  ...         1\n",
              "\n",
              "[19236 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7mm588oV1zK",
        "outputId": "1c23f71f-838c-44d2-fffa-e0bc59c47705"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88.02364430737599"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# pd.set_option('display.max_rows', df.shape[0]+1)\n",
        "neutral_df = visual_df.loc[visual_df.sentiment ==0,:]\n",
        "#neutral_df\n",
        "len_neutral_df = neutral_df.loc[:,['text','selected_text']].applymap(len)\n",
        "index_neutral = len_neutral_df.loc[len_neutral_df.text == len_neutral_df.selected_text,:]\n",
        "#cleaned_train_X.loc[index_neutral.index,:]\n",
        "(index_neutral.shape[0]/ neutral_df.shape[0])*100\n",
        "# pd.DataFrame(data ={'origin':cleaned_train_X.Raw_text,'text':cleaned_train_X.Cleaned_texts,'selected_text':preprocessed_train_y_ex.preprocessed_texts}).loc[index_neutral_diff.index,:]"
      ],
      "id": "k7mm588oV1zK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdPY3A5neVpH",
        "outputId": "454eb741-a28a-4707-d9df-4df22fe1c39b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25356</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9843</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11309</th>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>724</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12530</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13957</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2969</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24444</th>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9377</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2099 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       text  selected_text\n",
              "652       6              1\n",
              "25356     3              1\n",
              "9843      3              1\n",
              "11309    13              1\n",
              "724       7              1\n",
              "...     ...            ...\n",
              "12530     9              1\n",
              "13957     8              1\n",
              "2969     16              1\n",
              "24444    21              1\n",
              "9377     12              1\n",
              "\n",
              "[2099 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "pos_df = visual_df.loc[visual_df.sentiment ==-1\n",
        "                       ,:]\n",
        "len_pos_df = pos_df.loc[:,['text','selected_text']].applymap(len)\n",
        "# len_pos_df.selected_text.value_counts(normalize= True)*100\n",
        "len_pos_df.loc[len_pos_df.selected_text == 1,:]"
      ],
      "id": "QdPY3A5neVpH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHzR4cPKvshU"
      },
      "outputs": [],
      "source": [
        "# pos_visual_df = visual_df.loc[visual_df.sentiment == 1,:]\n",
        "# pos_visual_df.text.apply(len).max()"
      ],
      "id": "YHzR4cPKvshU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nfI3h5N5urf"
      },
      "source": [
        "#### 2.2 Viết class và tinh chỉnh mô hình"
      ],
      "id": "7nfI3h5N5urf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQnfsTiR4bMI"
      },
      "source": [
        "##### 2.2.1 Sentiment Classification Class"
      ],
      "id": "cQnfsTiR4bMI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUj52lCMtxqA"
      },
      "outputs": [],
      "source": [
        "class Sentiment_Classification(object):\n",
        "  def __init__(self,_max_depth = 3):\n",
        "        #self.num_top_titles = num_top_titles\n",
        "        self._max_depth = _max_depth\n",
        "  def extract_features(self,tweet, likelihood):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a dictionary of words for one tweet\n",
        "        likelihood: a dictionary corresponding to the score representation of each word\n",
        "    Output: \n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    word_l = list(tweet.values())\n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3))   \n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "        if likelihood.get(word,0)!=0:\n",
        "          # increment the word count for the positive label 1\n",
        "          x[0,0] += likelihood[word][0]\n",
        "        \n",
        "          # increment the word count for the negative label 0\n",
        "          x[0,1] += likelihood[word][1]\n",
        "\n",
        "          x[0,2] += likelihood[word][2]\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x\n",
        "  def sentence_embeddings(self,preprocessed_X_df,likelihood):\n",
        "    sentences = preprocessed_X_df.preprocessed_texts.to_list()\n",
        "    X = np.zeros(shape = (len(sentences),3))\n",
        "    for i in range(len(sentences)):\n",
        "      X[i, :]= self.extract_features(sentences[i], likelihood)\n",
        "    assert(X.shape == (len(sentences), 3))\n",
        "    return X\n",
        "  def _fit(self,preprocessed_y_ex,likelihood):\n",
        "    # freqs_temp_train = tse_st.count_tweets(preprocessed_train_X_y)\n",
        "    # vob_temp_train, likelihood_temp_train = tse_st.score_representation(freqs_temp_train,preprocessed_train_X_y)\n",
        "    self._likelihood = likelihood\n",
        "    train_X_TC = self.sentence_embeddings(preprocessed_y_ex,likelihood)\n",
        "    train_y_TC = np.array(preprocessed_y_ex.sentiment.to_list())\n",
        "    clf_RT = RandomForestClassifier(max_depth=self._max_depth, random_state=40)\n",
        "    #clf = SVC(gamma='auto',probability = True,random_state=1)\n",
        "    self.sentiment_tool = clf_RT.fit(train_X_TC,train_y_TC)\n",
        "    self.RT_score = clf_RT.score(train_X_TC,train_y_TC)\n",
        "    #self.score_train = self.sentiment_tool.score(X,y)\n",
        "    return self\n",
        "  def _predict(self,embeddings_X):\n",
        "    # predict_word_temp = np.array(likelihood_temp['on']).reshape(1,3)\n",
        "    # predict_tweet_temp = extract_features({0: 'hate'}\t\t,likelihood_temp)\n",
        "    result = self.sentiment_tool.predict(embeddings_X)\n",
        "    return result"
      ],
      "id": "cUj52lCMtxqA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayzn2hhwzMPg"
      },
      "outputs": [],
      "source": [
        "# preprocessed_train_X_y = pd.concat([preprocessed_train_X.loc[:,['preprocessed_texts','sentiment']],preprocessed_train_y_ex ],ignore_index= True)\n",
        "# s_temp = Sentiment_Classification()\n",
        "# result_temp = s_temp._fit(preprocessed_train_X_y,tse_st.likelihood)"
      ],
      "id": "ayzn2hhwzMPg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlxsEqaw316A"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "hlxsEqaw316A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g1O6RE_2j2h"
      },
      "outputs": [],
      "source": [
        "# result_temp.sentiment_tool.predict(predict_tweet_temp)"
      ],
      "id": "-g1O6RE_2j2h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqYF-DBe4inR"
      },
      "source": [
        "##### 2.2.2 Extraction Class"
      ],
      "id": "EqYF-DBe4inR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HyfLbZ5b0Am",
        "outputId": "baec87d8-be67-486d-b344-6f4d02254256"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['not',\n",
              " 'dont',\n",
              " 'doesnt',\n",
              " 'arent',\n",
              " 'isnt',\n",
              " 'havent',\n",
              " 'hasnt',\n",
              " 'didnt',\n",
              " 'wasnt',\n",
              " 'werent',\n",
              " 'hadnt',\n",
              " 'wont',\n",
              " 'shant',\n",
              " 'cant',\n",
              " 'couldnt',\n",
              " 'neitheir',\n",
              " 'nor',\n",
              " 'rare',\n",
              " 'hardli',\n",
              " 'seldom',\n",
              " 'never',\n",
              " 'no']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "negative_form = ['not','dont','doesnt','arent','isnt','havent','hasnt','didnt','wasnt','werent','hadnt','wont','shant','cant',\\\n",
        "'couldnt','neitheir','nor','rarely','hardly','seldom','never','no']\n",
        "negative_form = [stemmer.stem(w) for w in negative_form]\n",
        "negative_form"
      ],
      "id": "9HyfLbZ5b0Am"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQEkMeIAETLw"
      },
      "source": [
        "- Tạm thời thêm phần xử lý rút trích phủ định cho negative tweet TẠM THỜI như sau: \\\\ \n",
        "\n",
        "Xét từ phía trước nằm trong tập negative forms bên trên, nếu từ tiếp theo mang nghĩa positive thì trích nguyên cụm: negative form + positive word -> negative phrase \\\\\n",
        "\n",
        "Ví dụ: not + good -> not good -> negative\n",
        "\n",
        "\n"
      ],
      "id": "sQEkMeIAETLw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WSwp2TTTI5d"
      },
      "outputs": [],
      "source": [
        "afinn_wl_url = ('https://raw.githubusercontent.com'\n",
        "                '/fnielsen/afinn/master/afinn/data/AFINN-111.txt')\n",
        "\n",
        "afinn_wl_df = pd.read_csv(afinn_wl_url,\n",
        "                          header=None, # no column names\n",
        "                          sep='\\t',  # tab sepeated\n",
        "                          names=['term', 'value'])"
      ],
      "id": "8WSwp2TTTI5d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht_gqSUJsIKA",
        "outputId": "72aea2d6-b75e-4128-abd9-16a2a7dfa947"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "afinn.score('internet')"
      ],
      "id": "ht_gqSUJsIKA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpTYLpZBJuUj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class bayes_extraction(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, k = 5,z_neg=1.65,z_neu = 1.65,z_pos=1.65,w =1, vob_threshold = 2,_max_depth = 3):\n",
        "        #self.num_top_titles = num_top_titles\n",
        "        self.k = k\n",
        "        self.z_neg = z_neg\n",
        "        self.z_neu = z_neu\n",
        "        self.z_pos = z_pos\n",
        "        self.vob_threshold = vob_threshold\n",
        "        self.w = w\n",
        "        self._max_depth = _max_depth\n",
        "    def word2freqs(self,preprocessed_X):\n",
        "        preprocessed_tweets = preprocessed_X.preprocessed_texts.to_list()\n",
        "        list_of_tweets = [list(text.values()) for text in preprocessed_tweets ]\n",
        "        vob2freq = {}\n",
        "        for tw in list_of_tweets:\n",
        "            for word in tw:\n",
        "              if vob2freq.get(word,0) !=0:\n",
        "                  vob2freq[word] +=1\n",
        "              else:\n",
        "                  vob2freq[word] = 1\n",
        "        return vob2freq\n",
        "    def count_tweets(self,preprocessed_X):\n",
        "        '''\n",
        "          Input:\n",
        "      \n",
        "            tweets: a list of tweets\n",
        "            ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
        "          Output:\n",
        "              result: a dictionary mapping each pair to its frequency\n",
        "        '''\n",
        "        preprocessed_X = preprocessed_X.copy()\n",
        "        y_s = preprocessed_X.sentiment.to_list()\n",
        "        preprocessed_tweets = preprocessed_X.preprocessed_texts.to_list()\n",
        "        processed_tweets = [list(text.values()) for text in preprocessed_tweets ]\n",
        "\n",
        "\n",
        "        vob2freqs = self.word2freqs(preprocessed_X)\n",
        "        freqs = {}\n",
        "\n",
        "        for y, tweet in zip(y_s, processed_tweets):\n",
        "            for word in tweet:\n",
        "                if vob2freqs[word] < self.vob_threshold and y==0:\n",
        "                    pair = ('<OOV>',y)\n",
        "                else:\n",
        "\n",
        "                    # define the key, which is the word and label tuple\n",
        "                    pair = (word,y)\n",
        "\n",
        "              # if the key exists in the dictionary, increment the count\n",
        "                if pair in freqs:\n",
        "                    freqs[pair] += 1\n",
        "\n",
        "              # else, if the key is new, add it to the dictionary and set the count to 1\n",
        "                else:\n",
        "                    freqs[pair] = 1\n",
        "\n",
        "        return freqs\n",
        "    \n",
        "    def score_representation(self,freqs,preprocessed_X):\n",
        "        likelihood = {}\n",
        "        preprocessed_X = preprocessed_X.copy()\n",
        "        train_y = preprocessed_X.sentiment.to_list()\n",
        "        preprocessed_tweets = preprocessed_X.preprocessed_texts.to_list()\n",
        "        #train_x = [list(text.values()) for text in preprocessed_tweets ]\n",
        "    \n",
        "      \n",
        "        # calculate V, the number of unique words in the vocabulary\n",
        "        vocab = set([pair[0] for pair in freqs.keys()])\n",
        "        vocabs_ = list(vocab)\n",
        "\n",
        "        V = len(vocab)\n",
        "\n",
        "        # calculate N_pos and N_neg\n",
        "        N_pos = N_neg = N_neu = 0\n",
        "        for pair in freqs.keys():\n",
        "          # if the label is positive (greater than zero)\n",
        "            if pair[1] ==1:\n",
        "\n",
        "                # Increment the number of positive words by the count for this (word, label) pair\n",
        "                  N_pos += freqs[pair]\n",
        "            elif pair[1] ==0:\n",
        "                  N_neu += freqs[pair]\n",
        "            # else, the label is negative\n",
        "            else:\n",
        "                # increment the number of negative words by the count for this (word,label) pair\n",
        "                  N_neg += freqs[pair]\n",
        "\n",
        "          # Calculate D, the number of documents\n",
        "        D = len(train_y)\n",
        "\n",
        "        for word in vocab:\n",
        "          # get the positive and negative frequency of the word\n",
        "            freq_neg = freqs.get((word,-1),0)\n",
        "            freq_neu = freqs.get((word,0),0)\n",
        "            freq_pos = freqs.get((word,1),0)\n",
        "\n",
        "\n",
        "          # calculate the probability that each word is positive, and negative , neutral\n",
        "            s_neg = freq_neg / (freq_neg + freq_neu + freq_pos)\n",
        "            s_neu = freq_neu / (freq_neg + freq_neu + freq_pos)\n",
        "            s_pos = freq_pos / (freq_neg + freq_neu + freq_pos)\n",
        "\n",
        "          # calculate the log likelihood of the word\n",
        "            #likelihood[word] = [np.log(p_w_neg),np.log(p_w_neu),np.log(p_w_pos)]\n",
        "            likelihood[word] = [s_neg,s_neu,s_pos]\n",
        "        return vocabs_, likelihood\n",
        "        \n",
        "    \n",
        "    def bayes_likelihood(self,freqs,preprocessed_X):\n",
        "        '''\n",
        "          Input:\n",
        "              freqs: dictionary from (word, label) to how often the word appears\n",
        "          Output:\n",
        "              loglikelihood: the log likelihood of you Naive bayes equation.\n",
        "        '''\n",
        "        likelihood = {}\n",
        "        preprocessed_X = preprocessed_X.copy()\n",
        "        train_y = preprocessed_X.sentiment.to_list()\n",
        "        preprocessed_tweets = preprocessed_X.preprocessed_texts.to_list()\n",
        "        #train_x = [list(text.values()) for text in preprocessed_tweets ]\n",
        "    \n",
        "\n",
        "        # calculate V, the number of unique words in the vocabulary\n",
        "        vocab = set([pair[0] for pair in freqs.keys()])\n",
        "        vocabs_ = list(vocab)\n",
        "\n",
        "        V = len(vocab)\n",
        "\n",
        "        # calculate N_pos and N_neg\n",
        "        N_pos = N_neg = N_neu = 0\n",
        "        for pair in freqs.keys():\n",
        "          # if the label is positive (greater than zero)\n",
        "            if pair[1] ==1:\n",
        "\n",
        "                # Increment the number of positive words by the count for this (word, label) pair\n",
        "                  N_pos += freqs[pair]\n",
        "            elif pair[1] ==0:\n",
        "                  N_neu += freqs[pair]\n",
        "            # else, the label is negative\n",
        "            else:\n",
        "                # increment the number of negative words by the count for this (word,label) pair\n",
        "                  N_neg += freqs[pair]\n",
        "\n",
        "          # Calculate D, the number of documents\n",
        "        D = len(train_y)\n",
        "\n",
        "        for word in vocab:\n",
        "          # get the positive and negative frequency of the word\n",
        "            freq_pos = freqs.get((word,1),0)\n",
        "            freq_neu = freqs.get((word,0),0)\n",
        "            freq_neg = freqs.get((word,-1),0)\n",
        "        \n",
        "\n",
        "          # calculate the probability that each word is positive, and negative , neutral\n",
        "            p_w_pos = (freq_pos + self.k) / (N_pos + self.k*V)\n",
        "            p_w_neu = (freq_neu + self.k) / (N_neu + self.k*V)\n",
        "            p_w_neg = (freq_neg + self.k) / (N_neg + self.k*V)\n",
        "\n",
        "          # calculate the log likelihood of the word\n",
        "            #likelihood[word] = [np.log(p_w_neg),np.log(p_w_neu),np.log(p_w_pos)]\n",
        "            likelihood[word] = [p_w_neg,p_w_neu,p_w_pos]\n",
        "        return vocabs_, likelihood\n",
        "\n",
        "    def lexical_sentiment_score(self,preprocessed_y_ex,x_likelihood):\n",
        "        y_freqs = self.count_tweets(preprocessed_y_ex) # Dùng làm trọng số rút trích\n",
        "        #vocabs_,y_likelihood = self.bayes_likelihood(y_freqs,preprocessed_y_ex)\n",
        "        N_pos = N_neg = N_neu = 0\n",
        "        for pair in y_freqs.keys():\n",
        "          # if the label is positive (greater than zero)\n",
        "            if pair[1] ==1:\n",
        "\n",
        "                # Increment the number of positive words by the count for this (word, label) pair\n",
        "                  N_pos += y_freqs[pair]\n",
        "            elif pair[1] ==0:\n",
        "                  N_neu += y_freqs[pair]\n",
        "            # else, the label is negative\n",
        "            else:\n",
        "                # increment the number of negative words by the count for this (word,label) pair\n",
        "                  N_neg += y_freqs[pair]\n",
        "\n",
        "        new_x_likelihood = x_likelihood.copy()\n",
        "        for w,l in x_likelihood.items():\n",
        "            if y_freqs.get((w,-1),0)!=0:\n",
        "                #new_x_likelihood[w][0] = np.log((1/(-new_x_likelihood[w][0]- math.pow(10,-10))) * y_freqs[(w,-1)] )\n",
        "                new_x_likelihood[w][0] = new_x_likelihood[w][0]*(y_freqs[(w,-1)])*self.w\n",
        "            if y_freqs.get((w,0),0)!=0:\n",
        "                #new_x_likelihood[w][0] = np.log((1/(-new_x_likelihood[w][1] - math.pow(10,-10)  ))* y_freqs[(w,0)]  )\n",
        "                new_x_likelihood[w][1] = new_x_likelihood[w][1]*(y_freqs[(w,0)])*self.w\n",
        "            if y_freqs.get((w,1),0)!=0:\n",
        "                #new_x_likelihood[w][0] = np.log((1/(-new_x_likelihood[w][2]- math.pow(10,-10))) * y_freqs[(w,1)] )\n",
        "                new_x_likelihood[w][2] = new_x_likelihood[w][2]*(y_freqs[(w,1)])*self.w\n",
        "        return new_x_likelihood\n",
        "           \n",
        "    def sentiment_word_net(self,likelihood):\n",
        "          # word_net = np.zeros(shape = (len(likelihood),3))\n",
        "          # list_of_score = list(likelihood.values())\n",
        "          # for i in range(len(list_of_score)):\n",
        "          #       word_net[i] = np.array(list_of_score[i])\n",
        "          # preds_sentiment = model._predict(word_net)\n",
        "          # word2sentiment = {}\n",
        "          # words = list(likelihood.keys())\n",
        "          # for w,p in zip(words,list(preds_sentiment)):\n",
        "          #       word2sentiment[w] = p\n",
        "          # return word2sentiment\n",
        "          word2sentiment = {}\n",
        "          for w,score in likelihood.items():\n",
        "             max_prob = max(score)\n",
        "             word2sentiment[w] = score.index(max_prob) -1\n",
        "          return word2sentiment\n",
        "\n",
        "    def Extract_Sentiment_Tweet(self,x_class,tweet):\n",
        "          '''input:\n",
        "                tweet: list of word'''\n",
        "          selected_tweet = []\n",
        "          s = x_class +1\n",
        "          \n",
        "          if s==1:\n",
        "            for word in tweet:\n",
        "              if self.likelihood.get(word,0) !=0:\n",
        "                    neg_prob,neu_prob,pos_prob = self.likelihood[word]\n",
        "                    threshold = 1+self.z_neu\n",
        "                    if ((neg_prob/neu_prob) < 1 + self.z_neu) or ((pos_prob/neu_prob) < 1+ self.z_neu):\n",
        "                      selected_tweet.append(word)\n",
        "              else:\n",
        "                  selected_tweet.append(word)\n",
        "          elif s==2:\n",
        "              # for word in tweet:\n",
        "              #       if self.likelihood.get(word,0) !=0:\n",
        "              #           # word2emb = np.array(self.likelihood[word]).reshape(1,3)\n",
        "              #           # _sentiment = int(self.sentiment_tool._predict(word2emb)[0])\n",
        "              #           if self.word2sentiment[word] == x_class:\n",
        "              #                 selected_tweet.append(word)\n",
        "            for word in tweet:\n",
        "              if self.likelihood.get(word,0) !=0:\n",
        "                neg_prob,neu_prob,pos_prob = self.likelihood[word]\n",
        "                max_prob = max(self.likelihood[word])\n",
        "                if self.likelihood[word].index(max_prob) == s:\n",
        "                    if abs(max_prob -neu_prob) > self.z_pos:\n",
        "                    #if max_prob/neu_prob > self.z_pos:\n",
        "                        selected_tweet.append(word)\n",
        "          else:\n",
        "              for word_pos in range(len(tweet)):\n",
        "                if self.likelihood.get(tweet[word_pos],0) !=0:\n",
        "                    max_prob = max(self.likelihood[tweet[word_pos]])\n",
        "                    neg_prob,neu_prob,pos_prob = self.likelihood[tweet[word_pos]]\n",
        "                    if word_pos <len(tweet) -1:\n",
        "                      if tweet[word_pos] in negative_form and self.word2sentiment.get(tweet[word_pos+1] ,-2)== 1:\n",
        "                        selected_tweet.append(tweet[word_pos])\n",
        "                        selected_tweet.append(tweet[word_pos +1])\n",
        "\n",
        "                    if self.word2sentiment[tweet[word_pos]] == -1:\n",
        "                          if abs(max_prob -neu_prob) > self.z_neg:\n",
        "                    #if max_prob/neu_prob > self.z_pos:\n",
        "                              selected_tweet.append(tweet[word_pos])\n",
        "\n",
        "                \n",
        "          return selected_tweet\n",
        "\n",
        "    # def Extract_Sentiment_Tweet_Log(self,x_class,tweet):\n",
        "    #     '''input:\n",
        "    #       tweet: list of word'''\n",
        "    #     selected_tweet = []\n",
        "    #     s = x_class +1\n",
        "    #     if s == 0:\n",
        "    #       z = self.z_neg\n",
        "    #     elif s==1:\n",
        "    #       z = self.z_neu\n",
        "    #     elif s ==2:\n",
        "    #       z = self.z_pos\n",
        "    #     sentiment_score = 0\n",
        "    #     word_sentiment_score = {}\n",
        "    #     word_sentiment_percent = {}\n",
        "    #     for word in tweet:\n",
        "    #       if self.likelihood.get(word,0)!=0:\n",
        "    #             word_sentiment_score[word] = self.likelihood[word][s]\n",
        "    #             sentiment_score += self.likelihood[word][s]\n",
        "    #     # Tính phần trăm đóng góp sentiment của từng từ trong câu theo tỉ lệ phần trăm:\n",
        "    #     for w,score in word_sentiment_score.items():\n",
        "    #         percentage = (abs(score) / abs(sentiment_score))*100\n",
        "    #         word_sentiment_percent[w] = percentage\n",
        "    #     #print(word_sentiment_percent)\n",
        "    #     #threshold = sorted(list(word_sentiment_percent.values()),reverse= True)[0]\n",
        "    #     #len(list(word_sentiment_percent.values()))\n",
        "    #     data = np.array(list(word_sentiment_percent.values()))\n",
        "    #     if data.shape[0] !=0:\n",
        "    #         threshold = data.mean() + z*(data.std()/math.sqrt(data.shape[0]))\n",
        "    #     else:\n",
        "    #         threshold = 0\n",
        "    #     for w,percent in word_sentiment_percent.items():\n",
        "    #         if percent < threshold:\n",
        "    #             selected_tweet.append(w)\n",
        "\n",
        "    #     return selected_tweet\n",
        "    \n",
        "    def Predict_Extract_text(self,Preprocessed_X):\n",
        "        selected = []\n",
        "        preprocessed_X = Preprocessed_X.copy()\n",
        "        y_s = preprocessed_X.sentiment.to_list()\n",
        "        preprocessed_tweets = preprocessed_X.preprocessed_texts.to_list()\n",
        "        tweets = [list(text.values()) for text in preprocessed_tweets ]\n",
        "        for y, tweet in zip(y_s,tweets):\n",
        "            #selected_predict = Extract_Sentiment_Tweet(y,tweet,likelihood)\n",
        "            selected_predict = self.Extract_Sentiment_Tweet(y,tweet)\n",
        "            selected.append(selected_predict)\n",
        "        return selected\n",
        "    def Map_Extracted(self,Preprocessed_X,extracted_text):\n",
        "        # extracted_text = self.Predict_Extract_text(Preprocessed_X)\n",
        "        index2word = Preprocessed_X.index2word.to_list()\n",
        "        preprocessed_texts = Preprocessed_X.preprocessed_texts.to_list()\n",
        "        Selected_text = []\n",
        "        for e,i,p in zip(extracted_text,index2word,preprocessed_texts):\n",
        "            Text = {}\n",
        "            for j in p.items():\n",
        "              for w in e:\n",
        "                if w == j[1]:\n",
        "                  Text[j[0]] = i[j[0]]\n",
        "          \n",
        "            Selected_text.append(\" \".join(list(Text.values())))\n",
        "        \n",
        "        return Selected_text\n",
        "\n",
        "    def jaccard(self,str1, str2): \n",
        "        a = set(str1.lower().split()) \n",
        "        b = set(str2.lower().split())\n",
        "        c = a.intersection(b)\n",
        "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "    def fit(self, preprocessed_X_df, preprocessed_y_ex):\n",
        "        #preprocessed_train_X_y = pd.concat([preprocessed_X_df.loc[:,['preprocessed_texts','sentiment']],preprocessed_y_ex ],ignore_index= True)\n",
        "        self.freqs = self.count_tweets(preprocessed_X_df)\n",
        "        self.vocabs, self.init_likelihood = self.bayes_likelihood(self.freqs,preprocessed_X_df)\n",
        "        \n",
        "        #self.vocabs, self.likelihood = self.score_representation(self.freqs,preprocessed_train_X_y)\n",
        "        #self.vocabs, self.likelihood = self.bayes_likelihood(self.freqs,preprocessed_X_df)\n",
        "        self.likelihood = self.lexical_sentiment_score(preprocessed_y_ex,self.init_likelihood)\n",
        "        #self.sentiment_tool = Sentiment_Classification(_max_depth = self._max_depth)._fit(preprocessed_y_ex,self.likelihood)\n",
        "        #self.word2sentiment = self.sentiment_word_net(self.sentiment_tool,self.likelihood)\n",
        "        self.word2sentiment = self.sentiment_word_net(self.likelihood)\n",
        "        return self\n",
        "\n",
        "    def transform(self, preprocessed_X_df):\n",
        "        self.extracted_text = self.Predict_Extract_text(preprocessed_X_df)\n",
        "        selected_text = self.Map_Extracted(preprocessed_X_df,self.extracted_text)\n",
        "        return selected_text\n",
        "\n",
        "    def score(self,selected_text,y_ex):\n",
        "        score = 0\n",
        "        y_true_s = y_ex.copy().to_list()\n",
        "        for y_pred, y_true in zip(selected_text,y_true_s):\n",
        "            score += self.jaccard(str(y_true),y_pred)\n",
        "        return score/len(y_true_s)"
      ],
      "id": "cpTYLpZBJuUj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK8uiwX0s2Re"
      },
      "outputs": [],
      "source": [
        "#tse_st.count_tweets(preprocessed_train_y_ex)"
      ],
      "id": "EK8uiwX0s2Re"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqomATfa9FWv"
      },
      "source": [
        "##### 2.2.2.1 Training"
      ],
      "id": "hqomATfa9FWv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUSFMkrxizuV",
        "outputId": "0e259e67-c0a9-4b57-da87-6d4651b31933"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6249642898400228"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "# Training\n",
        "tse_st = bayes_extraction(z_pos= 0.00003,z_neu=4,z_neg = 0.00003,vob_threshold=2,k = 10)\n",
        "selected_train_X = tse_st.fit_transform(preprocessed_train_X,preprocessed_train_y_ex)\n",
        "#selected_train_X = tse_st.extract(preprocessed_train_X)\n",
        "tse_st.score(selected_train_X,train_y_ex)\n"
      ],
      "id": "MUSFMkrxizuV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84yreD1VMHvT"
      },
      "source": [
        "##### 2.2.2.2 Error Analysis"
      ],
      "id": "84yreD1VMHvT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1QiYAhzZTbT"
      },
      "outputs": [],
      "source": [
        "# score_df = pd.DataFrame(data ={'selected_text':selected_val_X ,'true_text':val_y_ex, 'sentiment':val_X_df.sentiment})\n",
        "# check_df = score_df.loc[score_df.sentiment== 'negative',:]\n",
        "# check_neg = check_df.selected_text.str.split().apply(len)\n",
        "# # check_neg = check_neg.loc[check_neg >=7].index\n",
        "# # check_df = check_df.loc[check_neg,:]\n",
        "# # tse_st.score(check_df.selected_text.to_list(),check_df.true_text)"
      ],
      "id": "K1QiYAhzZTbT"
    },
    {
      "cell_type": "code",
      "source": [
        "# check_df.selected_text.str.split().apply(len).value_counts()\n"
      ],
      "metadata": {
        "id": "bltqa2FVaDhb"
      },
      "id": "bltqa2FVaDhb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD925xy5MX8k"
      },
      "outputs": [],
      "source": [
        "# pd.set_option('display.max_rows', df.shape[0]+1)\n",
        "# compare_df = pd.DataFrame(data ={'origin':val_X_df.text,'preprocessed_text':preprocessed_val_X.preprocessed_texts,'selected_text':selected_val_X ,'true_text':val_y_ex, 'sentiment':val_X_df.sentiment})\n",
        "# index_check = compare_df.loc[compare_df.sentiment == 'negative',['selected_text']].selected_text.str.split().apply(len)\n",
        "# index_1 = index_check.loc[index_check == 1].index\n",
        "# #compare_df.loc[index_1,:]"
      ],
      "id": "JD925xy5MX8k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8By9glY82lX"
      },
      "source": [
        "##### 2.2.4 Test on val"
      ],
      "id": "z8By9glY82lX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKciMayZnRKD",
        "outputId": "8244976a-5962-46cc-f7c3-89e55f5e3629"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6137627000291629"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "# Test on val\n",
        "selected_val_X = tse_st.transform(preprocessed_val_X)\n",
        "tse_st.score(selected_val_X,val_y_ex)"
      ],
      "id": "TKciMayZnRKD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVquaeBt_c66"
      },
      "outputs": [],
      "source": [
        "#tse_st.likelihood['argi']"
      ],
      "id": "yVquaeBt_c66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC-3HOoSGxEJ",
        "outputId": "64e8c23a-3e34-484b-e1c8-05de352be665"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19247"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "len(tse_st.vocabs)"
      ],
      "id": "cC-3HOoSGxEJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r0MiMSQKYSs",
        "outputId": "9346fe48-ba7a-4800-e553-d25c5d9c4cd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.5085154585723785e-05, 0.000194209774300498, 0.00011038989711661589]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "tse_st.likelihood['crush']"
      ],
      "id": "3r0MiMSQKYSs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCCzLRAhtn8y"
      },
      "source": [
        "#### 2.3 Tunning"
      ],
      "id": "xCCzLRAhtn8y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5KtP1666NXW",
        "outputId": "8bdb230d-3000-4f68-9d08-ccf5a6a5e666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_train_score:  0.6281233127775028\n",
            "best_val_score:  0.6180062267115272\n",
            "best_k: 10 \n",
            " nbest_z_pos: 6\n"
          ]
        }
      ],
      "source": [
        "# Tunning\n",
        "#z_pos_s = [0.0001,0.0002,0.00003,0.00006,0.0004]\n",
        "vob_thresholds = [2,3,4,5,6]\n",
        "k_s = [1,2,5,7,10,12,15]\n",
        "Val_score = {}\n",
        "Train_score = {}\n",
        "\n",
        "for k in k_s:\n",
        "    #for z3 in z_pos_s:\n",
        "    for thres in vob_thresholds:\n",
        "      #model = bayes_extraction(z_pos=z3,k=k )\n",
        "      model = bayes_extraction(k=k,vob_threshold = thres,z_pos = 0.00003,z_neg = 0.00003, z_neu=4 )\n",
        "      selected_train_text= model.fit_transform(preprocessed_train_X,preprocessed_train_y_ex)\n",
        "      #selected_train_text = model.extract(preprocessed_train_X)\n",
        "      score_train = model.score(selected_train_text,train_y_ex)\n",
        "      #Train_score[score_train] = (k,z3)\n",
        "      Train_score[score_train] = (k,thres)\n",
        "      selected_val_X = model.transform(preprocessed_val_X)\n",
        "      score_val = model.score(selected_val_X,val_y_ex)\n",
        "      #Val_score[score_val] = (k,z3)\n",
        "      Val_score[score_val] = (k,thres)\n",
        "\n",
        "best_train_score = max(list(Train_score.keys()))\n",
        "best_val_score = max(list(Val_score.keys()))\n",
        "best_k,best_vob_thres = Val_score[best_val_score]\n",
        "print('best_train_score: ',best_train_score)\n",
        "print('best_val_score: ',best_val_score)\n",
        "print(f'best_k: {best_k} \\n nbest_z_pos: {best_vob_thres }')\n",
        "# best_val_score = max(Val_score)\n",
        "# best_z = z[Val_score.index(best_val_score)]\n",
        "# print('best_conf: ',best_z)\n",
        "# print('best_train_score: ',max(Train_score))\n",
        "# print('best_val_score: ',best_val_score)"
      ],
      "id": "z5KtP1666NXW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXoLlQLbXXYF"
      },
      "outputs": [],
      "source": [
        "#cleaned_train_X"
      ],
      "id": "wXoLlQLbXXYF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLcsECC8GwG6"
      },
      "source": [
        "## 3. Rút trích text trên tập test:"
      ],
      "id": "qLcsECC8GwG6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "044ff513",
        "outputId": "215e6d76-452c-47f1-bfaa-9080ab55bcc3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f87dea47db</td>\n",
              "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>96d74cb729</td>\n",
              "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eee518ae67</td>\n",
              "      <td>Recession hit Veronique Branquinho, she has to quit her company, such a shame!</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01082688c6</td>\n",
              "      <td>happy bday!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33987a8ee5</td>\n",
              "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3529</th>\n",
              "      <td>e5f0e6ef4b</td>\n",
              "      <td>its at 3 am, im very tired but i can`t sleep  but i try it</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3530</th>\n",
              "      <td>416863ce47</td>\n",
              "      <td>All alone in this old house again.  Thanks for the net which keeps me alive and kicking! Whoever invented the net, i wanna kiss your hair!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3531</th>\n",
              "      <td>6332da480c</td>\n",
              "      <td>I know what you mean. My little dog is sinking into depression... he wants to move someplace tropical</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3532</th>\n",
              "      <td>df1baec676</td>\n",
              "      <td>_sutra what is your next youtube video gonna be about? I love your videos!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3533</th>\n",
              "      <td>469e15c5a8</td>\n",
              "      <td>http://twitpic.com/4woj2 - omgssh  ang cute ng bby.!</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3534 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          textID  ... sentiment\n",
              "0     f87dea47db  ...   neutral\n",
              "1     96d74cb729  ...  positive\n",
              "2     eee518ae67  ...  negative\n",
              "3     01082688c6  ...  positive\n",
              "4     33987a8ee5  ...  positive\n",
              "...          ...  ...       ...\n",
              "3529  e5f0e6ef4b  ...  negative\n",
              "3530  416863ce47  ...  positive\n",
              "3531  6332da480c  ...  negative\n",
              "3532  df1baec676  ...  positive\n",
              "3533  469e15c5a8  ...  positive\n",
              "\n",
              "[3534 rows x 3 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df = pd.read_csv('Applied-Data-Science-Group-7/test.csv')\n",
        "test_df"
      ],
      "id": "044ff513"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b0c227d"
      },
      "outputs": [],
      "source": [
        "test_preprocessed,cleaned_test_df = preprocess_tweets(test_df)"
      ],
      "id": "4b0c227d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGJoIHWiGP-N",
        "outputId": "df5ede1b-4f67-41c4-87fd-88bbe3f046a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Last session of the day http://twitpic.com/67ezh',\n",
              " 'exciting Good',\n",
              " 'Recession shame!',\n",
              " 'happy',\n",
              " '']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds_extracted_text = model_full.extract(test_preprocessed)\n",
        "preds_extracted_text[0:5]"
      ],
      "id": "WGJoIHWiGP-N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7-jDX0EG5lg"
      },
      "source": [
        "## 4. Submission"
      ],
      "id": "k7-jDX0EG5lg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e7a7fbe"
      },
      "outputs": [],
      "source": [
        "submission_df = pd.DataFrame(data = {'textID': test_df.textID,'selected_text':pd.Series(preds_extracted_text)})"
      ],
      "id": "2e7a7fbe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e419516"
      },
      "outputs": [],
      "source": [
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "id": "5e419516"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "15374961",
        "outputId": "1951a3cc-373f-4cdb-f154-938b10756d11"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>selected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f87dea47db</td>\n",
              "      <td>Last session of the day http://twitpic.com/67ezh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>96d74cb729</td>\n",
              "      <td>exciting Good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eee518ae67</td>\n",
              "      <td>Recession shame!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01082688c6</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33987a8ee5</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID                                     selected_text\n",
              "0  f87dea47db  Last session of the day http://twitpic.com/67ezh\n",
              "1  96d74cb729                                     exciting Good\n",
              "2  eee518ae67                                  Recession shame!\n",
              "3  01082688c6                                             happy\n",
              "4  33987a8ee5                                                  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission_df.head(5)"
      ],
      "id": "15374961"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc856b24"
      },
      "outputs": [],
      "source": [
        "# y_test = test_df.sentiment.map({'positive':1,'neutral':0,'negative':-1}).to_list()\n",
        "# tweets_test = test_df.text.to_list()"
      ],
      "id": "dc856b24"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f524c5ee"
      },
      "outputs": [],
      "source": [
        "#Extract_text(y_test,preprocessing(tweets_test),likelihood)"
      ],
      "id": "f524c5ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17d994b6"
      },
      "outputs": [],
      "source": [
        "# submission = pd.DataFrame(data ={'textID': test_df.textID,'selected_text':pd.Series(extracted_text)})\n",
        "# predict"
      ],
      "id": "17d994b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc5ef3e4"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "bc5ef3e4"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8Weea1SUCBmz",
        "qLIRG_m1CdvD",
        "b6ca4fc3",
        "awyk6UXcC5gA",
        "ee0725cc",
        "2Mui8N94wDFm",
        "cQnfsTiR4bMI",
        "EqYF-DBe4inR",
        "84yreD1VMHvT",
        "OjewiSa78vMf",
        "xCCzLRAhtn8y",
        "dMz34722YX0P",
        "qLcsECC8GwG6",
        "k7-jDX0EG5lg"
      ],
      "name": "Copy of Report_TSE_BERT_TK.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 123.060019,
      "end_time": "2021-10-20T19:02:58.579178",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-10-20T19:00:55.519159",
      "version": "2.3.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}