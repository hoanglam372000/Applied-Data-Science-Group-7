{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_12_TXL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPSrH8QTKsz7J30TsVtU5S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoanglam372000/Applied-Data-Science-Group-7/blob/nguyenthiman/7_12_TXL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Weea1SUCBmz"
      },
      "source": [
        "#### 1.1.1 Thư viện:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-20T19:01:05.711080Z",
          "iopub.status.busy": "2021-10-20T19:01:05.709811Z",
          "iopub.status.idle": "2021-10-20T19:01:05.712966Z",
          "shell.execute_reply": "2021-10-20T19:01:05.713505Z",
          "shell.execute_reply.started": "2021-10-20T18:55:02.236174Z"
        },
        "id": "b0c2dbd1",
        "papermill": {
          "duration": 0.05292,
          "end_time": "2021-10-20T19:01:05.713836",
          "exception": false,
          "start_time": "2021-10-20T19:01:05.660916",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e9f192-aa09-41db-dc44-9209472fc18a"
      },
      "source": [
        "!git clone https://github.com/hoanglam372000/Applied-Data-Science-Group-7.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Applied-Data-Science-Group-7'...\n",
            "remote: Enumerating objects: 115, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (113/113), done.\u001b[K\n",
            "remote: Total 115 (delta 60), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (115/115), 4.86 MiB | 4.75 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-20T19:01:05.799619Z",
          "iopub.status.busy": "2021-10-20T19:01:05.798867Z",
          "iopub.status.idle": "2021-10-20T19:01:36.347459Z",
          "shell.execute_reply": "2021-10-20T19:01:36.346709Z",
          "shell.execute_reply.started": "2021-10-20T18:55:02.243151Z"
        },
        "id": "eff8fbcf",
        "papermill": {
          "duration": 30.592473,
          "end_time": "2021-10-20T19:01:36.347624",
          "exception": false,
          "start_time": "2021-10-20T19:01:05.755151",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778bbda8-a1eb-4f48-f149-f0929fd87331"
      },
      "source": [
        "!pip install emoji\n",
        "!pip install pyenchant\n",
        "#!pip install pycontractions"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 30 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 170 kB 5.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=09b788a3cc77a79e2ce0a094eacbbbf68e4287df1373c9da461cd2e6990eac04\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-10-20T19:01:36.440628Z",
          "iopub.status.busy": "2021-10-20T19:01:36.439872Z",
          "iopub.status.idle": "2021-10-20T19:01:38.454775Z",
          "shell.execute_reply": "2021-10-20T19:01:38.454041Z",
          "shell.execute_reply.started": "2021-10-20T18:55:11.353568Z"
        },
        "id": "9b92498d",
        "papermill": {
          "duration": 2.064979,
          "end_time": "2021-10-20T19:01:38.454947",
          "exception": false,
          "start_time": "2021-10-20T19:01:36.389968",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10325adf-5722-442a-925c-6468b563ef4d"
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import emoji\n",
        "#import contractions\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#print(stopwords.words('english'))\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('names')\n",
        "names = nltk.corpus.names.words('male.txt')+nltk.corpus.names.words('female.txt')\n",
        "names_lower = [name.lower() for name in names]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK7e9Cf_q-AD",
        "outputId": "86847c00-9ac9-4a77-92d5-87c03c7d46c5"
      },
      "source": [
        "!pip install afinn"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting afinn\n",
            "  Downloading afinn-0.1.tar.gz (52 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▎                         | 10 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 52 kB 780 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53448 sha256=e19b7feb22a2868983149467eac5a4544b941656fc2790760a565970b8efc763\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/16/3a/9f0953027434eab5dadf3f33ab3298fa95afa8292fcf7aba75\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2inlf1hrDkj"
      },
      "source": [
        "from afinn import Afinn\n",
        "afinn = Afinn(language='en')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImKmbWuCU9Ve"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOZrwO3_VACX"
      },
      "source": [
        "# TXL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WG_adjiU9S2"
      },
      "source": [
        "# icon phổ biến\n",
        "icon_df=pd.read_csv('icon.csv')\n",
        "icon=icon_df['icon'].to_list()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MwGdq-AU0fZ"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "# class xử lý vấn đề trùng ký tự trong từ, ví dụ yoooouuuu -> you, nooooo -> no.\n",
        "#sử dụng pattern của regular expression, tuy nhiên phải quét thêm qua wordnet để tránh trường hợp như từ goose bị chuyển thành gose (mặc dù goose là từ đúng)\n",
        "class RepeatReplacer(object):\n",
        "    def __init__(self):\n",
        "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "        self.repl = r'\\1\\2\\3'\n",
        "    def replace(self, word):\n",
        "        if wordnet.synsets(word):\n",
        "            return word\n",
        "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
        "        if repl_word != word:\n",
        "            return self.replace(repl_word)\n",
        "        else:\n",
        "            return repl_word\n",
        "char_repeat_correct = RepeatReplacer()\n",
        "def preprocessing_word(word):\n",
        "  if word != emoji.demojize(word):\n",
        "    return emoji.demojize(word)\n",
        "  #word = contractions.fix(word)\n",
        "  #word = stemmer.stem(word)\n",
        "  temp=[ \"<html_tag>\",'<URL>','<HIDDEN>','<tag>',\"<hash_tag>\"]\n",
        "  if word in temp:\n",
        "    return word\n",
        "  #số theo từ\n",
        "  if word.isdigit():  \n",
        "    return '<NUMBER>'\n",
        "  try: \n",
        "    float(word)\n",
        "    return '<NUMBER>'\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  #word=word.lower() # viết thườngthường\n",
        "  #spell_corrector = Speller(lang='en') #sửa chính tả theo từng từ \n",
        "  #word=spell_corrector(word)\n",
        "\n",
        "  # rút gọn theo từ\n",
        "  #lemmatizer = WordNetLemmatizer()\n",
        "  #wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "  #pos_tagged = nltk.pos_tag(word.split())\n",
        "  \n",
        "  #word = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged]\n",
        "  #word = ' '.join(word)\n",
        "\n",
        "  #word=word.translate(str.maketrans('', '', string.punctuation)) # chấm câu theo từ (vì tránh emoij)\n",
        "  word = re.sub(pattern=r'[!\"#$%&\\'()*+,-\\./:;<=>?@\\[\\]^_`{|}~0-9ï¿½]+', repl='', string=word)\n",
        "  word = char_repeat_correct.replace(word)\n",
        "  word = stemmer.stem(word)\n",
        "  if (word in  names):\n",
        "    return '<Name>'\n",
        "  #word = stemmer.stem(word)\n",
        "  word = word.lower()\n",
        "  \n",
        "  if word==''or word ==' ':\n",
        "    return \"<Special_Character>\"\n",
        "  \n",
        "  if 'not' not in word and len(word.split())!=1:\n",
        "    return word.split()[0]+'_'\n",
        "  return word.replace(' ','_')\n",
        "\n",
        "def preprocessing_text(tweet,sentiment):\n",
        "   raw_text = str(tweet).strip().replace('\\n', ' ')#.lower()\n",
        "   # emoij\n",
        "   raw_text=emoji.demojize(raw_text)\n",
        "   e_r=r':\\w+_\\w+:'\n",
        "   emoij_list=re.findall(e_r,raw_text)\n",
        "   for e in emoij_list:\n",
        "     raw_text=raw_text.replace(e,' ' + emoji.emojize(e) + ' ') # \"hi❤️👍\"\n",
        "   if sentiment !=0: \n",
        "     raw_text = re.sub(pattern=r'(?<=\\w)(\\.){2,}?(?=\\w)', repl=' ', string=raw_text)\n",
        "   new_tweet = raw_text.split()\n",
        "   len_=len(new_tweet)\n",
        "   index2word = { w:new_tweet[w] for w in range(len_)}\n",
        "   #parser = BeautifulSoup(raw_text, \"html.parser\")\n",
        "   #raw_text = parser.get_text(separator = \"<html_tag>\")\n",
        "   raw_text = re.sub(pattern=r'https?://\\S+|www\\.\\S+', repl='<URL>', string=raw_text)\n",
        "   raw_text = re.sub(pattern='\\*{3,}?', repl='<HIDDEN>', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'@\\w+', repl='<tag>', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'#\\w+', repl='<hash_tag>', string=raw_text)\n",
        "   \n",
        "   word_list=raw_text.split()\n",
        "   return index2word,dict(zip([*range(len_)],map(preprocessing_word,word_list)))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR-8WyVJWWHC",
        "outputId": "fffe6bf5-8585-478a-b514-a50a44b4755e"
      },
      "source": [
        "def preprocessing_string(tweet,sentiment):\n",
        "   raw_text = str(tweet).strip().replace('\\n', ' ')#.lower()\n",
        "   # emoij\n",
        "   raw_text=emoji.demojize(raw_text)\n",
        "   e_r=r':\\w+_\\w+:'\n",
        "   emoij_list=re.findall(e_r,raw_text)\n",
        "   for e in emoij_list:\n",
        "     raw_text=raw_text.replace(e,' ' + emoji.emojize(e) + ' ') # \"hi❤️👍\"\n",
        "   if sentiment !=0: \n",
        "     raw_text = re.sub(pattern=r'(?<=\\w)(\\.){2,}?(?=\\w)', repl=' ', string=raw_text)\n",
        "   \n",
        "  #  r = re.sub(r\"[#!\\\"$%&\\'*+,-\\.?@\\[\\]():;/`{|}]*\",'',w[1])\n",
        "  #  if r == '' or r ==' ':\n",
        "  #    raw_text=raw_text.replace(e,' ' + emoji.emojize(e) + ' ')\n",
        "  #  else:\n",
        "  #    preprocessed_tweet[w[0]] = r\n",
        "\n",
        "   new_tweet = raw_text.split()\n",
        "   len_=len(new_tweet)\n",
        "   index2word = { w:new_tweet[w] for w in range(len_)}\n",
        "   #parser = BeautifulSoup(raw_text, \"html.parser\")\n",
        "   #raw_text = parser.get_text(separator = \"<html_tag>\")\n",
        "   raw_text = re.sub(pattern=r'https?://\\S+|www\\.\\S+', repl='<URL>', string=raw_text)\n",
        "   raw_text = re.sub(pattern='\\*{3,}?', repl='<HIDDEN>', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'@\\w+', repl='<tag>', string=raw_text)\n",
        "   raw_text = re.sub(pattern=r'#\\w+', repl='<hash_tag>', string=raw_text)\n",
        "   \n",
        "   word_list=raw_text.split()\n",
        "   return word_list\n",
        "\n",
        "preprocessing_string('hi:))',0) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi:))']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "dn6LTk8yYaFe",
        "outputId": "1343f764-b2ee-44ad-f3b8-8b653148b1b7"
      },
      "source": [
        "w=\"hi:))\"\n",
        "s=r\"[#!\\\"$%&\\'*+,-\\.?@\\[\\]():;/`{|}]*\"\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' [#!\\\\\"$%&\\\\\\'*+,-\\\\.?@\\\\[\\\\]():;/`{|}]*h [#!\\\\\"$%&\\\\\\'*+,-\\\\.?@\\\\[\\\\]():;/`{|}]*i [#!\\\\\"$%&\\\\\\'*+,-\\\\.?@\\\\[\\\\]():;/`{|}]* [#!\\\\\"$%&\\\\\\'*+,-\\\\.?@\\\\[\\\\]():;/`{|}]*'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ctrRct-VkeK",
        "outputId": "c8a2e09b-dee2-4001-f50b-e520674e231e"
      },
      "source": [
        "# Test:\n",
        "tweet = ' My:)) mom just texted.......me and told *ship* James that Rodney3 charlie hi❤️👍 wasï¿½ *** chasing?? and fireflies... in their :)) backyard. Awwwww I`m miss him!!! @hoanglam 123 ://    http://twitpic.com/66pn1'\n",
        "tweet2 = 'nooooooooo'\n",
        "index2word_temp,preprocessed_tweet_temp = preprocessing_text(tweet,0)\n",
        "print('index2word: ',index2word_temp)\n",
        "print('preprocessed_tweet: ',preprocessed_tweet_temp)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index2word:  {0: 'My:))', 1: 'mom', 2: 'just', 3: 'texted.......me', 4: 'and', 5: 'told', 6: '*ship*', 7: 'James', 8: 'that', 9: 'Rodney3', 10: 'charlie', 11: 'hi', 12: '❤️', 13: '👍', 14: 'wasï¿½', 15: '***', 16: 'chasing??', 17: 'and', 18: 'fireflies...', 19: 'in', 20: 'their', 21: ':))', 22: 'backyard.', 23: 'Awwwww', 24: 'I`m', 25: 'miss', 26: 'him!!!', 27: '@hoanglam', 28: '123', 29: '://', 30: 'http://twitpic.com/66pn1'}\n",
            "preprocessed_tweet:  {0: 'my', 1: 'mom', 2: 'just', 3: 'textedm', 4: 'and', 5: 'told', 6: 'ship', 7: 'jame', 8: 'that', 9: 'rodney', 10: 'charli', 11: 'hi', 12: ':red_heart:', 13: ':thumbs_up:', 14: 'wa', 15: '<HIDDEN>', 16: 'chase', 17: 'and', 18: 'firefli', 19: 'in', 20: 'their', 21: '<Special_Character>', 22: 'backyard', 23: 'aw', 24: 'im', 25: 'miss', 26: 'him', 27: '<tag>', 28: '<NUMBER>', 29: '<Special_Character>', 30: '<URL>'}\n"
          ]
        }
      ]
    }
  ]
}